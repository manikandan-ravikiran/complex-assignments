{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TOPIC RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code runs TOPICRANK model for extracting keyphrases and it doesnt need any TF-IDF computation\n",
    "    \n",
    "* The input testdata will need complete test file, rather than sentences like supervised models.  This can be modified using line ```extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',```\n",
    "\n",
    "* Also to have comparable results we do fuzzy matching with sentence where we extracted, so to produce results for diifferent folds. Please change the names of test folds as per your requirement. See line ```df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")```. Folds for files include **test1.tsv** and **test2.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# this example uses TopicRank\n",
    "from pke.unsupervised import TopicRank\n",
    "from collections import Counter\n",
    "# create a TopicRank extractor\n",
    "extractor = TopicRank()\n",
    "\n",
    "# load the content of the document, here in CoreNLP XML format\n",
    "# the input language is set to English (used for the stoplist)\n",
    "# normalization is set to stemming (computed with Porter's stemming algorithm)\n",
    "extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',\n",
    "                        language=\"en\",\n",
    "                        normalization='stemming')\n",
    "\n",
    "# select the keyphrase candidates, for TopicRank the longest sequences of \n",
    "# nouns and adjectives\n",
    "extractor.candidate_selection(pos={'NOUN', 'PROPN', 'ADJ'})\n",
    "\n",
    "# weight the candidates using a random walk. The threshold parameter sets the\n",
    "# minimum similarity for clustering, and the method parameter defines the \n",
    "# linkage method\n",
    "extractor.candidate_weighting(threshold=0.1,\n",
    "                              method='average')\n",
    "allkeyphrases=[]\n",
    "# print the n-highest (10) scored candidates\n",
    "\n",
    "\n",
    "\n",
    "for (keyphrase, score) in extractor.get_n_best(n=10, stemming=False):\n",
    "    allkeyphrases.append(keyphrase)\n",
    "\n",
    "df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")\n",
    "texts=df[\"text\"]\n",
    "texts=[i.replace(\",\",\"\") for i in texts]\n",
    "labels=df[\"label\"]\n",
    "labels=[0 if i==0 else 1 for i in labels]\n",
    "\n",
    "overall_evidence=[]\n",
    "removed_text=[]\n",
    "for txt in texts:\n",
    "    evidence=[]\n",
    "    for phr in allkeyphrases :\n",
    "        ##Fuzzy Matching\n",
    "        if phr in txt and txt not in removed_text:\n",
    "            evidence.append(1)\n",
    "            removed_text.append(txt)\n",
    "    if evidence==[]:\n",
    "        evidence=[0]\n",
    "    overall_evidence.append(evidence)\n",
    "\n",
    "ypred=[Counter(i).most_common(1)[0][0] for i in overall_evidence]\n",
    "# print(ypred)\n",
    "\n",
    "from sklearn.metrics import classification_report,f1_score\n",
    "print(f1_score(labels,ypred,average='macro'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEXT RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code runs TEXTRANK model for extracting keyphrases and it doesnt need any TF-IDF computation\n",
    "    \n",
    "* The input testdata will need complete test file, rather than sentences like supervised models.  This can be modified using line ```extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',```\n",
    "\n",
    "* Also to have comparable results we do fuzzy matching with sentence where we extracted, so to produce results for diifferent folds. Please change the names of test folds as per your requirement. See line ```df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")```. Folds for files include **test1.tsv** and **test2.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# this example uses TopicRank\n",
    "from pke.unsupervised import TopicRank\n",
    "from collections import Counter\n",
    "import pke\n",
    "# this example uses TopicRank\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a TextRank extractor.\n",
    "extractor = pke.unsupervised.TextRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. build the graph representation of the document and rank the words.\n",
    "#    Keyphrase candidates are composed from the 33-percent\n",
    "#    highest-ranked words.\n",
    "extractor.candidate_weighting(window=2,\n",
    "                              pos=pos,\n",
    "                              top_percent=0.33)\n",
    "\n",
    "\n",
    "# print the n-highest (10) scored candidates\n",
    "\n",
    "\n",
    "allkeyphrases=[]\n",
    "# print the n-highest (10) scored candidates\n",
    "for (keyphrase, score) in extractor.get_n_best(n=200, stemming=False):\n",
    "    allkeyphrases.append(keyphrase)\n",
    "\n",
    "df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")\n",
    "texts=df[\"text\"]\n",
    "texts=[i.replace(\",\",\"\") for i in texts]\n",
    "labels=df[\"label\"]\n",
    "labels=[0 if i==0 else 1 for i in labels]\n",
    "\n",
    "overall_evidence=[]\n",
    "removed_text=[]\n",
    "for txt in texts:\n",
    "    evidence=[]\n",
    "    for phr in allkeyphrases :\n",
    "         ##Fuzzy Matching\n",
    "        if phr in txt and txt not in removed_text:\n",
    "            evidence.append(1)\n",
    "            removed_text.append(txt)\n",
    "    if evidence==[]:\n",
    "        evidence=[0]\n",
    "    overall_evidence.append(evidence)\n",
    "\n",
    "ypred=[Counter(i).most_common(1)[0][0] for i in overall_evidence]\n",
    "print(ypred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels,ypred,digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SINGLE RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code runs SINGLERANK model for extracting keyphrases and it doesnt need any TF-IDF computation\n",
    "    \n",
    "* The input testdata will need complete test file, rather than sentences like supervised models.  This can be modified using line ```extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',```\n",
    "\n",
    "* Also to have comparable results we do fuzzy matching with sentence where we extracted, so to produce results for diifferent folds. Please change the names of test folds as per your requirement. See line ```df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")```. Folds for files include **test1.tsv** and **test2.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "\n",
    "# define the set of valid Part-of-Speeches\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# 1. create a SingleRank extractor.\n",
    "extractor = pke.unsupervised.SingleRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select the longest sequences of nouns and adjectives as candidates.\n",
    "extractor.candidate_selection(pos=pos)\n",
    "\n",
    "# 4. weight the candidates using the sum of their word's scores that are\n",
    "#    computed using random walk. In the graph, nodes are words of\n",
    "#    certain part-of-speech (nouns and adjectives) that are connected if\n",
    "#    they occur in a window of 10 words.\n",
    "extractor.candidate_weighting(window=10,\n",
    "                              pos=pos)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "allkeyphrases=[]\n",
    "# print the n-highest (10) scored candidates\n",
    "for (keyphrase, score) in extractor.get_n_best(n=100, stemming=False):\n",
    "    allkeyphrases.append(keyphrase)\n",
    "\n",
    "df=pd.read_csv(\"../train_data/tsv/test2.tsv\",delimiter=\"\\t\")\n",
    "texts=df[\"text\"]\n",
    "texts=[i.replace(\",\",\"\") for i in texts]\n",
    "labels=df[\"label\"]\n",
    "labels=[0 if i==0 else 1 for i in labels]\n",
    "\n",
    "overall_evidence=[]\n",
    "removed_text=[]\n",
    "for txt in texts:\n",
    "    evidence=[]\n",
    "    for phr in allkeyphrases :\n",
    "         ##Fuzzy Matching\n",
    "        if phr in txt and txt not in removed_text:\n",
    "            evidence.append(1)\n",
    "            removed_text.append(txt)\n",
    "    if evidence==[]:\n",
    "        evidence=[0]\n",
    "    overall_evidence.append(evidence)\n",
    "\n",
    "ypred=[Counter(i).most_common(1)[0][0] for i in overall_evidence]\n",
    "print(ypred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels,ypred,digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSITION RANK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code runs POSITIONRANK model for extracting keyphrases and it doesnt need any TF-IDF computation\n",
    "    \n",
    "* The input testdata will need complete test file, rather than sentences like supervised models.  This can be modified using line ```extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',```\n",
    "\n",
    "* Also to have comparable results we do fuzzy matching with sentence where we extracted, so to produce results for diifferent folds. Please change the names of test folds as per your requirement. See line ```df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")```. Folds for files include **test1.tsv** and **test2.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "\n",
    "# define the valid Part-of-Speeches to occur in the graph\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "\n",
    "# define the grammar for selecting the keyphrase candidates\n",
    "grammar = \"NP: {<ADJ>*<NOUN|PROPN>+}\"\n",
    "\n",
    "# 1. create a PositionRank extractor.\n",
    "extractor = pke.unsupervised.PositionRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "# 3. select the noun phrases up to 3 words as keyphrase candidates.\n",
    "extractor.candidate_selection(grammar=grammar,\n",
    "                              maximum_word_number=5)\n",
    "\n",
    "# 4. weight the candidates using the sum of their word's scores that are\n",
    "#    computed using random walk biaised with the position of the words\n",
    "#    in the document. In the graph, nodes are words (nouns and\n",
    "#    adjectives only) that are connected if they occur in a window of\n",
    "#    10 words.\n",
    "extractor.candidate_weighting(window=10,\n",
    "                              pos=pos)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "allkeyphrases=[]\n",
    "# print the n-highest (10) scored candidates\n",
    "for (keyphrase, score) in extractor.get_n_best(n=200, stemming=False):\n",
    "    allkeyphrases.append(keyphrase)\n",
    "\n",
    "df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")\n",
    "texts=df[\"text\"]\n",
    "texts=[i.replace(\",\",\"\") for i in texts]\n",
    "labels=df[\"label\"]\n",
    "labels=[0 if i==0 else 1 for i in labels]\n",
    "\n",
    "overall_evidence=[]\n",
    "removed_text=[]\n",
    "for txt in texts:\n",
    "    evidence=[]\n",
    "    for phr in allkeyphrases :\n",
    "         ##Fuzzy Matching\n",
    "        if phr in txt and txt not in removed_text:\n",
    "            evidence.append(1)\n",
    "            removed_text.append(txt)\n",
    "    if evidence==[]:\n",
    "        evidence=[0]\n",
    "    overall_evidence.append(evidence)\n",
    "\n",
    "ypred=[Counter(i).most_common(1)[0][0] for i in overall_evidence]\n",
    "print(ypred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels,ypred,digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIPARTITE RANKING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code runs MULTIPARTITERANK model for extracting keyphrases and it doesnt need any TF-IDF computation\n",
    "    \n",
    "* The input testdata will need complete test file, rather than sentences like supervised models.  This can be modified using line ```extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',```\n",
    "\n",
    "* Also to have comparable results we do fuzzy matching with sentence where we extracted, so to produce results for diifferent folds. Please change the names of test folds as per your requirement. See line ```df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")```. Folds for files include **test1.tsv** and **test2.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 1. create a MultipartiteRank extractor.\n",
    "extractor = pke.unsupervised.MultipartiteRank()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt')\n",
    "\n",
    "# 3. select the longest sequences of nouns and adjectives, that do\n",
    "#    not contain punctuation marks or stopwords as candidates.\n",
    "pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "stoplist = list(string.punctuation)\n",
    "stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "stoplist += stopwords.words('english')\n",
    "extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
    "\n",
    "# 4. build the Multipartite graph and rank candidates using random walk,\n",
    "#    alpha controls the weight adjustment mechanism, see TopicRank for\n",
    "#    threshold/method parameters.\n",
    "extractor.candidate_weighting(alpha=1.1,\n",
    "                              threshold=0.74,\n",
    "                              method='average')\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "allkeyphrases=[]\n",
    "# print the n-highest (10) scored candidates\n",
    "for (keyphrase, score) in extractor.get_n_best(n=10, stemming=False):\n",
    "    allkeyphrases.append(keyphrase)\n",
    "\n",
    "df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test1.tsv\",delimiter=\"\\t\")\n",
    "texts=df[\"text\"]\n",
    "texts=[i.replace(\",\",\"\") for i in texts]\n",
    "labels=df[\"label\"]\n",
    "labels=[0 if i==0 else 1 for i in labels]\n",
    "\n",
    "overall_evidence=[]\n",
    "removed_text=[]\n",
    "for txt in texts:\n",
    "    evidence=[]\n",
    "    for phr in allkeyphrases :\n",
    "         ##Fuzzy Matching\n",
    "        if phr in txt and txt not in removed_text:\n",
    "            evidence.append(1)\n",
    "            removed_text.append(txt)\n",
    "    if evidence==[]:\n",
    "        evidence=[0]\n",
    "    overall_evidence.append(evidence)\n",
    "\n",
    "ypred=[Counter(i).most_common(1)[0][0] for i in overall_evidence]\n",
    "print(ypred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels,ypred,digits=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
