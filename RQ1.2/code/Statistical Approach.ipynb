{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KPMINER MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code runs KPMINER model for extracting keyphrases, please note that KPMINER requires TF-IDF. For this you need to run the first cell from *https://github.com/manikandan-ravikiran/cs6460-proj/blob/master/RQ1.1/code/KEA.ipynb*\n",
    "    \n",
    "* The testdata willl need complete test file, rather than sentences like supervised models.  This can be modified using line ```extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',```\n",
    "\n",
    "* * Also to have comparable results we do fuzzy matching with sentence where we extracted, so to produce results for diifferent folds. Please change the names of test folds as per your requirement. See line ```df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")```. Folds for files include **test1.tsv** and **test2.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "# 1. create a KPMiner extractor.\n",
    "extractor = pke.unsupervised.KPMiner()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "\n",
    "# 3. select {1-5}-grams that do not contain punctuation marks or\n",
    "#    stopwords as keyphrase candidates. Set the least allowable seen\n",
    "#    frequency to 5 and the number of words after which candidates are\n",
    "#    filtered out to 200.\n",
    "lasf = 5\n",
    "cutoff = 200\n",
    "extractor.candidate_selection(lasf=lasf, cutoff=cutoff)\n",
    "\n",
    "# 4. weight the candidates using KPMiner weighting function.\n",
    "df = pke.load_document_frequency_file(input_file='../data/df_kea_test.tsv.gz')\n",
    "alpha = 2.3\n",
    "sigma = 3.0\n",
    "extractor.candidate_weighting(df=df, alpha=alpha, sigma=sigma)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases\n",
    "allkeyphrases=[]\n",
    "# print the n-highest (10) scored candidates\n",
    "for (keyphrase, score) in extractor.get_n_best(n=100, stemming=False):\n",
    "    allkeyphrases.append(keyphrase)\n",
    "\n",
    "df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")\n",
    "texts=df[\"text\"]\n",
    "texts=[i.replace(\",\",\"\") for i in texts]\n",
    "labels=df[\"label\"]\n",
    "labels=[0 if i==0 else 1 for i in labels]\n",
    "\n",
    "overall_evidence=[]\n",
    "removed_text=[]\n",
    "for txt in texts:\n",
    "    evidence=[]\n",
    "    for phr in allkeyphrases :\n",
    "        #Fuzzy matching\n",
    "        if phr in txt and txt not in removed_text:\n",
    "            evidence.append(1)\n",
    "            removed_text.append(txt)\n",
    "    if evidence==[]:\n",
    "        evidence=[0]\n",
    "    overall_evidence.append(evidence)\n",
    "\n",
    "ypred=[Counter(i).most_common(1)[0][0] for i in overall_evidence]\n",
    "print(ypred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels,ypred,digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAKE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code runs YAKE model for extracting keyphrases and it doesnt need any TF-IDF computation\n",
    "    \n",
    "* The input testdata will need complete test file, rather than sentences like supervised models.  This can be modified using line ```extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',```\n",
    "\n",
    "* Also to have comparable results we do fuzzy matching with sentence where we extracted, so to produce results for diifferent folds. Please change the names of test folds as per your requirement. See line ```df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")```. Folds for files include **test1.tsv** and **test2.tsv**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 1. create a YAKE extractor.\n",
    "extractor = pke.unsupervised.YAKE()\n",
    "\n",
    "# 2. load the content of the document.\n",
    "extractor.load_document(input='../../RQ1.1/train_data/document/test/test.txt',\n",
    "                        language='en',\n",
    "                        normalization=None)\n",
    "\n",
    "\n",
    "# 3. select {1-3}-grams not containing punctuation marks and not\n",
    "#    beginning/ending with a stopword as candidates.\n",
    "stoplist = stopwords.words('english')\n",
    "extractor.candidate_selection(n=3, stoplist=stoplist)\n",
    "\n",
    "# 4. weight the candidates using YAKE weighting scheme, a window (in\n",
    "#    words) for computing left/right contexts can be specified.\n",
    "window = 2\n",
    "use_stems = False # use stems instead of words for weighting\n",
    "extractor.candidate_weighting(window=window,\n",
    "                              stoplist=stoplist,\n",
    "                              use_stems=use_stems)\n",
    "\n",
    "# 5. get the 10-highest scored candidates as keyphrases.\n",
    "#    redundant keyphrases are removed from the output using levenshtein\n",
    "#    distance and a threshold.\n",
    "allkeyphrases=[]\n",
    "# print the n-highest (10) scored candidates\n",
    "for (keyphrase, score) in extractor.get_n_best(n=100, stemming=False):\n",
    "    allkeyphrases.append(keyphrase)\n",
    "\n",
    "df=pd.read_csv(\"../../RQ1.1/train_data/tsv/test2.tsv\",delimiter=\"\\t\")\n",
    "texts=df[\"text\"]\n",
    "texts=[i.replace(\",\",\"\") for i in texts]\n",
    "labels=df[\"label\"]\n",
    "labels=[0 if i==0 else 1 for i in labels]\n",
    "\n",
    "overall_evidence=[]\n",
    "removed_text=[]\n",
    "for txt in texts:\n",
    "    evidence=[]\n",
    "    for phr in allkeyphrases :\n",
    "        #Fuzzy matching\n",
    "        if phr in txt and txt not in removed_text:\n",
    "            evidence.append(1)\n",
    "            removed_text.append(txt)\n",
    "    if evidence==[]:\n",
    "        evidence=[0]\n",
    "    overall_evidence.append(evidence)\n",
    "\n",
    "ypred=[Counter(i).most_common(1)[0][0] for i in overall_evidence]\n",
    "print(ypred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels,ypred,digits=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
