{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "trainer_robert_edtech.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fa8ded99567b4f45835ab6497de25ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d3b6a9386f0440cdb99371e69cc01b62",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b3c94b1750844b128b70dd2f4cc2a823",
              "IPY_MODEL_04b35dd744f0406fb916dce91800e087"
            ]
          }
        },
        "d3b6a9386f0440cdb99371e69cc01b62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b3c94b1750844b128b70dd2f4cc2a823": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d8a93f3059aa45f0865796d611353e67",
            "_dom_classes": [],
            "description": "Evaluating",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 184,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 184,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_1593a9d326c9484d9d6af02e666a4983"
          }
        },
        "04b35dd744f0406fb916dce91800e087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_95028519633243b4bc54edd123e255d4",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 184/184 [00:10&lt;00:00, 18.09it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6f1efde09769464dbfaaca20b286ceaf"
          }
        },
        "d8a93f3059aa45f0865796d611353e67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "1593a9d326c9484d9d6af02e666a4983": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "95028519633243b4bc54edd123e255d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6f1efde09769464dbfaaca20b286ceaf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c31864a936540b89f42db0dc21e794f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0a40c124a6e849f9872bbe80dd2955d7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_7fec7b827818456d9b91ab6e47e6933a",
              "IPY_MODEL_46b48d1afe37435188dc9e71d57ecd3e"
            ]
          }
        },
        "0a40c124a6e849f9872bbe80dd2955d7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7fec7b827818456d9b91ab6e47e6933a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_90e72cc301ef45d7a71fe132a3c52625",
            "_dom_classes": [],
            "description": "Evaluating",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 276,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 276,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_53fb87ea5a4148cab0b5dbb5568c6c83"
          }
        },
        "46b48d1afe37435188dc9e71d57ecd3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a38c0448cff7455ba99170aae9d4d6c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 276/276 [00:14&lt;00:00, 18.62it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ee4922f7a95484f98d1d312873ff12d"
          }
        },
        "90e72cc301ef45d7a71fe132a3c52625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "53fb87ea5a4148cab0b5dbb5568c6c83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a38c0448cff7455ba99170aae9d4d6c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ee4922f7a95484f98d1d312873ff12d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0YLoS0hWz-ch",
        "scrolled": true,
        "outputId": "dad80d7c-1240-460e-dab6-cb7eea28d786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install torch torchvision pytorch-transformers\n",
        "!pip install scikit-learn pandas tensorflow lime\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "import torch\n",
        "\n",
        "torch.manual_seed(0)\n",
        "import csv\n",
        "import logging\n",
        "import os\n",
        "import sys\n",
        "from io import open\n",
        "\n",
        "from scipy.stats import pearsonr, spearmanr\n",
        "from sklearn.metrics import matthews_corrcoef, f1_score\n",
        "\n",
        "from multiprocessing import Pool, cpu_count\n",
        "from tqdm import tqdm\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "csv.field_size_limit(2147483647)\n",
        "# writerobj = SummaryWriter()\n",
        "\n",
        "class InputExample(object):\n",
        "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
        "\n",
        "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
        "        \"\"\"Constructs a InputExample.\n",
        "\n",
        "        Args:\n",
        "            guid: Unique id for the example.\n",
        "            text_a: string. The untokenized text of the first sequence. For single\n",
        "            sequence tasks, only this sequence must be specified.\n",
        "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
        "            Only must be specified for sequence pair tasks.\n",
        "            label: (Optional) string. The label of the example. This should be\n",
        "            specified for train and dev examples, but not for test examples.\n",
        "        \"\"\"\n",
        "        self.guid = guid\n",
        "        self.text_a = text_a\n",
        "        self.text_b = text_b\n",
        "        self.label = label\n",
        "\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
        "        self.input_ids = input_ids\n",
        "        self.input_mask = input_mask\n",
        "        self.segment_ids = segment_ids\n",
        "        self.label_id = label_id\n",
        "\n",
        "\n",
        "class DataProcessor(object):\n",
        "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_dev_examples(self, data_dir):\n",
        "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    @classmethod\n",
        "    def _read_tsv(cls, input_file, quotechar=None):\n",
        "        \"\"\"Reads a tab separated value file.\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
        "            lines = []\n",
        "            for line in reader:\n",
        "                if sys.version_info[0] == 2:\n",
        "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
        "                lines.append(line)\n",
        "            return lines\n",
        "\n",
        "\n",
        "class BinaryProcessor(DataProcessor):\n",
        "    \"\"\"Processor for the binary data sets\"\"\"\n",
        "\n",
        "    def get_train_examples(self, data_dir):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
        "\n",
        "    def get_dev_examples(self, data_dir,data_type=\"dev\"):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return self._create_examples(\n",
        "            self._read_tsv(os.path.join(data_dir, data_type+\".tsv\")), \"dev\")\n",
        "\n",
        "    def get_labels(self):\n",
        "        \"\"\"See base class.\"\"\"\n",
        "        return [\"0\",\"1\",\"2\",\"3\"]\n",
        "\n",
        "    def _create_examples(self, lines, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, line) in enumerate(lines):\n",
        "            \n",
        "            guid = \"%s-%s\" % (set_type, i)\n",
        "            try:\n",
        "                text_a = line[3]\n",
        "                label = line[1]\n",
        "            except:\n",
        "                print(line)\n",
        "            #print(text_a,label)\n",
        "            examples.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "        return examples\n",
        "\n",
        "\n",
        "def convert_example_to_feature(example_row, pad_token=0,\n",
        "sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "mask_padding_with_zero=True):\n",
        "    \n",
        "   \n",
        "    \n",
        "    example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id = example_row\n",
        "    #print(example.text_a,\"label-------->\",example.label)\n",
        "    tokens_a = tokenizer.tokenize(example.text_a)\n",
        "\n",
        "    tokens_b = None\n",
        "    if example.text_b:\n",
        "        tokens_b = tokenizer.tokenize(example.text_b)\n",
        "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
        "        # length is less than the specified length.\n",
        "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
        "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
        "    else:\n",
        "        # Account for [CLS] and [SEP] with \"- 2\"\n",
        "        if len(tokens_a) > max_seq_length - 2:\n",
        "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
        "\n",
        "    # The convention in BERT is:\n",
        "    # (a) For sequence pairs:\n",
        "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
        "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
        "    # (b) For single sequences:\n",
        "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
        "    #  type_ids:   0   0   0   0  0     0   0\n",
        "    #\n",
        "    # Where \"type_ids\" are used to indicate whether this is the first\n",
        "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
        "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
        "    # embedding vector (and position vector). This is not *strictly* necessary\n",
        "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
        "    # it easier for the model to learn the concept of sequences.\n",
        "    #\n",
        "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
        "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
        "    # the entire model is fine-tuned.\n",
        "    tokens = tokens_a + [sep_token]\n",
        "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
        "\n",
        "    if tokens_b:\n",
        "        tokens += tokens_b + [sep_token]\n",
        "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
        "\n",
        "    if cls_token_at_end:\n",
        "        tokens = tokens + [cls_token]\n",
        "        segment_ids = segment_ids + [cls_token_segment_id]\n",
        "    else:\n",
        "        tokens = [cls_token] + tokens\n",
        "        segment_ids = [cls_token_segment_id] + segment_ids\n",
        "\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
        "    # tokens are attended to.\n",
        "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
        "\n",
        "    # Zero-pad up to the sequence length.\n",
        "    padding_length = max_seq_length - len(input_ids)\n",
        "    if pad_on_left:\n",
        "        input_ids = ([pad_token] * padding_length) + input_ids\n",
        "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
        "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
        "    else:\n",
        "        input_ids = input_ids + ([pad_token] * padding_length)\n",
        "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
        "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
        "\n",
        "    assert len(input_ids) == max_seq_length\n",
        "    assert len(input_mask) == max_seq_length\n",
        "    assert len(segment_ids) == max_seq_length\n",
        "    \n",
        "    #print(label_map)\n",
        "\n",
        "    if output_mode == \"classification\":\n",
        "        label_id = label_map[example.label]\n",
        "    elif output_mode == \"regression\":\n",
        "        label_id = float(example.label)\n",
        "    else:\n",
        "        raise KeyError(output_mode)\n",
        "\n",
        "    return InputFeatures(input_ids=input_ids,\n",
        "                        input_mask=input_mask,\n",
        "                        segment_ids=segment_ids,\n",
        "                        label_id=label_id)\n",
        "    \n",
        "\n",
        "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
        "                                 tokenizer, output_mode,\n",
        "                                 cls_token_at_end=False, pad_on_left=False,\n",
        "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
        "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
        "                                 cls_token_segment_id=1, pad_token_segment_id=0,\n",
        "                                 mask_padding_with_zero=True,\n",
        "                                 process_count=cpu_count() - 2):\n",
        "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
        "        `cls_token_at_end` define the location of the CLS token:\n",
        "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
        "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
        "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
        "    \"\"\"\n",
        "\n",
        "    label_map = {label : i for i, label in enumerate(label_list)}\n",
        "    #print(label_map)\n",
        "\n",
        "    examples = [(example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id) for example in examples]\n",
        "\n",
        "    with Pool(process_count) as p:\n",
        "        features = list(tqdm(p.imap(convert_example_to_feature, examples, chunksize=100), total=len(examples)))\n",
        "\n",
        "    return features\n",
        "\n",
        "\n",
        "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
        "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
        "\n",
        "    # This is a simple heuristic which will always truncate the longer sequence\n",
        "    # one token at a time. This makes more sense than truncating an equal percent\n",
        "    # of tokens from each, since if one sequence is very short then each token\n",
        "    # that's truncated likely contains more information than a longer sequence.\n",
        "    while True:\n",
        "        total_length = len(tokens_a) + len(tokens_b)\n",
        "        if total_length <= max_length:\n",
        "            break\n",
        "        if len(tokens_a) > len(tokens_b):\n",
        "            tokens_a.pop()\n",
        "        else:\n",
        "            tokens_b.pop()\n",
        "\n",
        "\n",
        "processors = {\n",
        "    \"binary\": BinaryProcessor\n",
        "}\n",
        "\n",
        "output_modes = {\n",
        "    \"binary\": \"classification\"\n",
        "}\n",
        "\n",
        "GLUE_TASKS_NUM_LABELS = {\n",
        "    \"binary\": 4\n",
        "}\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
            "Collecting pytorch-transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1.0MB 27.2MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 870kB 34.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.11.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.14.15)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (2.6.1)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=df81d73ceaa35461e66889744220f02a63753daeff87347b782d6452ee376380\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, sacremoses, pytorch-transformers\n",
            "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.38 sentencepiece-0.1.85\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Collecting lime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/e0/60070b461a589b2fee0dbc45df9987f150fca83667c2f8a064cef7dbac6b/lime-0.1.1.37.tar.gz (275kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276kB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
            "Collecting progressbar\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/a6/b8e451f6cff1c99b4747a2f7235aa904d2d49e8e1464e0b798272aa84358/progressbar-2.5.tar.gz\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime) (3.1.3)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime) (0.16.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (45.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.2.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (1.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (0.10.0)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (6.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.1)\n",
            "Building wheels for collected packages: lime, progressbar\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.1.1.37-cp36-none-any.whl size=284277 sha256=c9e75f44ad68f5d25cabe1541713cdd60d58a62c7775fdbc0e618970f65ce0ee\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/38/e7/50d75d4fb75afa604570dc42f20c5c5f5ab26d3fbe8d6ef27b\n",
            "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progressbar: filename=progressbar-2.5-cp36-none-any.whl size=12074 sha256=9a5a4c62ed3b4c445eec44f931f12538e825113ff2721215d9d5fd73e03628fa\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/e9/6b/ea01090205e285175842339aa3b491adeb4015206cda272ff0\n",
            "Successfully built lime progressbar\n",
            "Installing collected packages: progressbar, lime\n",
            "Successfully installed lime-0.1.1.37 progressbar-2.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVJDURgfcGf2",
        "colab_type": "text"
      },
      "source": [
        "# Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qYv4tGNs4Tg8",
        "scrolled": true,
        "outputId": "8d74fc6d-c16d-4f3c-9eae-6a4f51d53eb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm_notebook\n",
        "import io\n",
        "from google.colab import files\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# trainfile = files.upload()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be9yhGBXkit1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df = pd.read_csv(\"/content/drive/My Drive/exp1/train1.tsv\",delimiter=\"\\t\")\n",
        "test_df = pd.read_csv(\"/content/drive/My Drive/exp1/test1.tsv\",delimiter=\"\\t\")\n",
        "dev_df = test_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "0mmUcSnZi3cv",
        "colab_type": "code",
        "outputId": "eeb1348c-e1a9-44e8-80f0-d115c88eeaf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        }
      },
      "source": [
        "train_df"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>This version of the agent was able to perform ...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>For a 2*2 matrix, the agent initially checks w...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The time complexity remains the same as previo...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>At the same time, there is no inductivelearnin...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thereafter, generalizing algorithms by avoidin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>271</th>\n",
              "      <td>To beginwith, from BP-2, it can be seen that i...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>272</th>\n",
              "      <td>Analysisof problems reveal that rule ATowardsI...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>273</th>\n",
              "      <td>Problems CP-4 was solved, withoutany other err...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>274</th>\n",
              "      <td>Based on analysis from submission8, for this s...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>275</th>\n",
              "      <td>Results revealed that it can alone solved6 BPâ€™...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>276 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  label\n",
              "0    This version of the agent was able to perform ...      2\n",
              "1    For a 2*2 matrix, the agent initially checks w...      1\n",
              "2    The time complexity remains the same as previo...      2\n",
              "3    At the same time, there is no inductivelearnin...      2\n",
              "4    Thereafter, generalizing algorithms by avoidin...      1\n",
              "..                                                 ...    ...\n",
              "271  To beginwith, from BP-2, it can be seen that i...      2\n",
              "272  Analysisof problems reveal that rule ATowardsI...      3\n",
              "273  Problems CP-4 was solved, withoutany other err...      2\n",
              "274  Based on analysis from submission8, for this s...      1\n",
              "275  Results revealed that it can alone solved6 BPâ€™...      4\n",
              "\n",
              "[276 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hVsnPNzr4XjB",
        "scrolled": true,
        "outputId": "6e93f548-b21c-4f5b-8815-5f5a26bf005b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "train_df = pd.DataFrame({\n",
        "    'id':range(len(train_df)),\n",
        "    'label':train_df[\"label\"].astype(int)-1,\n",
        "    'alpha':['a']*train_df.shape[0],\n",
        "    'text': train_df[\"text\"].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>alpha</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>This version of the agent was able to perform ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>For a 2*2 matrix, the agent initially checks w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>The time complexity remains the same as previo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>At the same time, there is no inductivelearnin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>Thereafter, generalizing algorithms by avoidin...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label alpha                                               text\n",
              "0   0      1     a  This version of the agent was able to perform ...\n",
              "1   1      0     a  For a 2*2 matrix, the agent initially checks w...\n",
              "2   2      1     a  The time complexity remains the same as previo...\n",
              "3   3      1     a  At the same time, there is no inductivelearnin...\n",
              "4   4      0     a  Thereafter, generalizing algorithms by avoidin..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IkUFgZeh4Xf2",
        "scrolled": true,
        "outputId": "bc573d62-9e0b-404e-bcc3-a1b73b89efea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "dev_df = pd.DataFrame({\n",
        "    'id':range(len(dev_df)),\n",
        "    'label':dev_df[\"label\"].astype(int)-1,\n",
        "    'alpha':['a']*dev_df.shape[0],\n",
        "    'text': dev_df[\"text\"].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "dev_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>alpha</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>I feel that my agent is similar to me in some ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>This possibly indicates that the update I made...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>This checks if the difference in black pixels ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>It now computes thecombination of two images u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Most importantly the performance on test set a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label alpha                                               text\n",
              "0   0      1     a  I feel that my agent is similar to me in some ...\n",
              "1   1      1     a  This possibly indicates that the update I made...\n",
              "2   2      0     a  This checks if the difference in black pixels ...\n",
              "3   3      0     a  It now computes thecombination of two images u...\n",
              "4   4      1     a  Most importantly the performance on test set a..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "rhQDq4-Gi3dJ",
        "colab_type": "code",
        "outputId": "61ec3f17-9e18-4cd2-edb6-4c985867715e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "source": [
        "test_df = pd.DataFrame({\n",
        "    'id':range(len(test_df)),\n",
        "     'label':test_df[\"label\"].astype(int)-1,\n",
        "    'alpha':['a']*test_df.shape[0],\n",
        "    'text': test_df[\"text\"].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "test_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>label</th>\n",
              "      <th>alpha</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>I feel that my agent is similar to me in some ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>This possibly indicates that the update I made...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>This checks if the difference in black pixels ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>a</td>\n",
              "      <td>It now computes thecombination of two images u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>a</td>\n",
              "      <td>Most importantly the performance on test set a...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id  label alpha                                               text\n",
              "0   0      1     a  I feel that my agent is similar to me in some ...\n",
              "1   1      1     a  This possibly indicates that the update I made...\n",
              "2   2      0     a  This checks if the difference in black pixels ...\n",
              "3   3      0     a  It now computes thecombination of two images u...\n",
              "4   4      1     a  Most importantly the performance on test set a..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pFlvoH234XdO",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "!mkdir data_new_1\n",
        "train_df.to_csv('data_new_1/train.tsv', sep='\\t', index=False, header=False, columns=train_df.columns)\n",
        "dev_df.to_csv('data_new_1/dev.tsv', sep='\\t', index=False, header=False, columns=dev_df.columns)\n",
        "test_df.to_csv('data_new_1/test.tsv', sep='\\t', index=False, header=False, columns=test_df.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VeXuXWylz7BD",
        "scrolled": true,
        "outputId": "b184995a-8da1-4d8a-8e81-2f8a264c65a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "!pip install pytorch_transformers\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.1.85)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.11.15)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.17.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.0.38)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.3.3)\n",
            "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.14.15)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_transformers) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_transformers) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cN3zjpIdi3dc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
        "                              TensorDataset)\n",
        "import random\n",
        "from torch.utils.data.distributed import DistributedSampler\n",
        "from tqdm import tqdm_notebook, trange\n",
        "\n",
        "\n",
        "from pytorch_transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer,\n",
        "                                  XLMConfig, XLMForSequenceClassification, XLMTokenizer, \n",
        "                                  XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n",
        "                                  RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "\n",
        "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
        "\n",
        "# from utils import (convert_examples_to_features,\n",
        "#                         output_modes, processors)\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "F93_pIopz7BG",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "args_roberta = {\n",
        "    'data_dir': 'data_new_1/',\n",
        "    'model_type':  'roberta',\n",
        "    'model_name': 'roberta-base',\n",
        "    'task_name': 'binary',\n",
        "    'output_dir': 'outputs_robert-base-0.9_taskbc',\n",
        "    'cache_dir': 'cache_roberta/',\n",
        "    'do_train': True,\n",
        "    'do_eval': True,\n",
        "    'fp16': False,\n",
        "    'fp16_opt_level': 'O1',\n",
        "    'max_seq_length': 128,\n",
        "    'output_mode': 'classification',\n",
        "    'train_batch_size': 16,\n",
        "    'eval_batch_size': 1,\n",
        "\n",
        "    'gradient_accumulation_steps': 1,\n",
        "    'num_train_epochs': 10,\n",
        "    'weight_decay': 0,\n",
        "    'learning_rate': 4e-8,\n",
        "    'adam_epsilon': 1e-8,\n",
        "    'warmup_steps': 1000,\n",
        "    'max_grad_norm': 1.0,\n",
        "\n",
        "    'logging_steps': 50,\n",
        "    'evaluate_during_training': False,\n",
        "    'save_steps': 5000,\n",
        "    'eval_all_checkpoints': True,\n",
        "\n",
        "    'overwrite_output_dir': False,\n",
        "    'reprocess_input_data': False,\n",
        "    'notes': 'Using Yelp Reviews dataset'\n",
        "}\n",
        "args_bert=args_roberta\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Uzr2RwGLz7BL",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "with open('args_bert.json', 'w') as f:\n",
        "    json.dump(args_bert, f)\n",
        "with open('args_roberta.json', 'w') as f:\n",
        "    json.dump(args_roberta, f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ymjmIyOhz7BN",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "if os.path.exists(args_bert['output_dir']) and os.listdir(args_bert['output_dir']) and args_bert['do_train'] and not args_bert['overwrite_output_dir']:\n",
        "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args_bert['output_dir']))\n",
        "\n",
        "#Create roberta directory\n",
        "if os.path.exists(args_roberta['output_dir']) and os.listdir(args_roberta['output_dir']) and args_roberta['do_train'] and not args_roberta['overwrite_output_dir']:\n",
        "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args_roberta['output_dir']))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LAHYiiLMz7BP",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
        "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "\n",
        "config_class_bert, model_class_bert, tokenizer_class_bert = MODEL_CLASSES[args_bert['model_type']]\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
        "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
        "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
        "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
        "}\n",
        "\n",
        "#roberta\n",
        "config_class_roberta, model_class_roberta, tokenizer_class_roberta = MODEL_CLASSES[args_roberta['model_type']]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qm5AguwFz7BR",
        "scrolled": true,
        "outputId": "fb056d80-a88b-4b5f-aa0e-cbd693b25026",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# config_bert = config_class_bert.from_pretrained(args_bert['model_name'], num_labels=4, finetuning_task=args_bert['task_name'])#, output_hidden_states=True)\n",
        "# tokenizer_bert = tokenizer_class_bert.from_pretrained(args_bert['model_name'])\n",
        "\n",
        "config_bert=config_roberta = config_class_roberta.from_pretrained(args_roberta['model_name'], num_labels=4, finetuning_task=args_roberta['task_name'])\n",
        "tokenizer_bert=tokenizer_roberta = tokenizer_class_roberta.from_pretrained(args_roberta['model_name'])\n",
        "model_bert=model_roberta= model_class_roberta.from_pretrained(args_roberta['model_name'],num_labels=4)\n",
        "model_bert.to(device);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json not found in cache or force_download set to True, downloading to /tmp/tmpahqs4dax\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 524/524 [00:00<00:00, 183917.60B/s]\n",
            "INFO:pytorch_transformers.file_utils:copying /tmp/tmpahqs4dax to cache at /root/.cache/torch/pytorch_transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpahqs4dax\n",
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/pytorch_transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": \"binary\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 4,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json not found in cache or force_download set to True, downloading to /tmp/tmpgcnbqegv\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 898823/898823 [00:00<00:00, 5844102.32B/s]\n",
            "INFO:pytorch_transformers.file_utils:copying /tmp/tmpgcnbqegv to cache at /root/.cache/torch/pytorch_transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpgcnbqegv\n",
            "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt not found in cache or force_download set to True, downloading to /tmp/tmpgspakpt9\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 456318/456318 [00:00<00:00, 3350294.98B/s]\n",
            "INFO:pytorch_transformers.file_utils:copying /tmp/tmpgspakpt9 to cache at /root/.cache/torch/pytorch_transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpgspakpt9\n",
            "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-vocab.json from cache at /root/.cache/torch/pytorch_transformers/d0c5776499adc1ded22493fae699da0971c1ee4c2587111707a4d177d20257a2.ef00af9e673c7160b4d41cfda1f48c5f4cba57d5142754525572a846a1ab1b9b\n",
            "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-merges.txt from cache at /root/.cache/torch/pytorch_transformers/b35e7cd126cd4229a746b5d5c29a749e8e84438b14bcdb575950584fe33207e8.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n",
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/pytorch_transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 4,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmp2hl2c7_c\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 501200538/501200538 [00:09<00:00, 50142515.88B/s]\n",
            "INFO:pytorch_transformers.file_utils:copying /tmp/tmp2hl2c7_c to cache at /root/.cache/torch/pytorch_transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmp2hl2c7_c\n",
            "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "INFO:pytorch_transformers.modeling_utils:Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "INFO:pytorch_transformers.modeling_utils:Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IGZHNvKAz7BU",
        "scrolled": true,
        "outputId": "328f9d4d-e647-4355-fac2-7361781e91f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        }
      },
      "source": [
        "model_bert = model_class_roberta.from_pretrained(args_bert['model_name'],num_labels=4)\n",
        "# for name, param in model_bert.named_parameters():\n",
        "# \tif 'classifier' not in name: # classifier layer\n",
        "# \t\tparam.requires_grad = False"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-config.json from cache at /root/.cache/torch/pytorch_transformers/e1a2a406b5a05063c31f4dfdee7608986ba7c6393f7f79db5e69dcd197208534.a7ab0e5de2d8321d6d6a15b199110f2c99be72976b7d151423cb8d8c261a13b6\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"architectures\": [\n",
            "    \"RobertaForMaskedLM\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"finetuning_task\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"max_position_embeddings\": 514,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"num_labels\": 4,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"pruned_heads\": {},\n",
            "  \"torchscript\": false,\n",
            "  \"type_vocab_size\": 1,\n",
            "  \"vocab_size\": 50265\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/roberta-base-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/228756ed15b6d200d7cb45aaef08c087e2706f54cb912863d2efe07c89584eb7.49b88ba7ec2c26a7558dda98ca3884c3b80fa31cf43a1b1f23aef3ff81ba344e\n",
            "INFO:pytorch_transformers.modeling_utils:Weights of RobertaForSequenceClassification not initialized from pretrained model: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
            "INFO:pytorch_transformers.modeling_utils:Weights from pretrained model not used in RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xyxKpk_6z7BW",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "\n",
        "model_bert.to(device);\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bsPmRyGE8GnR",
        "scrolled": true,
        "outputId": "86d6ee6a-7120-4dfb-da91-a80a703039cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Xe4P94Bfz7Ba",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "task = args_bert['task_name']\n",
        "\n",
        "processor = processors[task]()\n",
        "label_list = processor.get_labels()\n",
        "num_labels = len(label_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xqr_fwM3z7Bd",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "def load_and_cache_examples(task, tokenizer,argus, evaluate=False,undersample_scale_factor=1,data_type=\"dev\"):\n",
        "    args=argus.copy()\n",
        "    processor = processors[task]()\n",
        "    output_mode = args['output_mode']\n",
        "    \n",
        "    mode = 'dev' if evaluate else 'train'\n",
        "    cached_features_file = os.path.join(args['data_dir'], \"cached_\"+str(mode)+\"_\"+str(args['model_name'])+\"_\"+str(args['max_seq_length'])+\"_\"+str(task))\n",
        "    \n",
        "    if os.path.exists(cached_features_file) and not args['reprocess_input_data'] and False:\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "               \n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n",
        "        label_list = processor.get_labels()\n",
        "        examples = processor.get_dev_examples(args['data_dir'],data_type) if evaluate else processor.get_train_examples(args['data_dir'])\n",
        "#         print(len(examples))\n",
        "        examples  = [example for example in examples]\n",
        "        #print((examples))\n",
        "        \n",
        "        features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer, output_mode,\n",
        "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),            # xlnet has a cls token at the end\n",
        "            cls_token=tokenizer.cls_token,\n",
        "            sep_token=tokenizer.sep_token,\n",
        "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
        "            pad_on_left=bool(args['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n",
        "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0,\n",
        "            process_count=2)\n",
        "        \n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "        \n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    if output_mode == \"classification\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "    elif output_mode == \"regression\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oCul6vvCz7Bg",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "def train(train_dataset, model, tokenizer,argus):\n",
        "    global writerobj\n",
        "    args=argus.copy()\n",
        "    train_sampler = RandomSampler(train_dataset)\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
        "    \n",
        "    t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
        "    \n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "        ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
        "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n",
        "    \n",
        "    if args['fp16']:\n",
        "        try:\n",
        "            from apex import amp\n",
        "        except ImportError:\n",
        "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
        "        model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n",
        "        \n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
        "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "\n",
        "    global_step = 0\n",
        "    tr_loss, logging_loss = 0.0, 0.0\n",
        "    model.zero_grad()\n",
        "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
        "    \n",
        "    for _ in train_iterator:\n",
        "        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(device) for t in batch)\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            #print(model.bert(**inputs).shape)\n",
        "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
        "            print(\"\\r%f\" % loss, end='')\n",
        "            #writerobj.add_scalar('Loss/train', float(loss), global_step)\n",
        "            \n",
        "\n",
        "            if args['gradient_accumulation_steps'] > 1:\n",
        "                loss = loss / args['gradient_accumulation_steps']\n",
        "                \n",
        "               \n",
        "                \n",
        " \n",
        "            if args['fp16']:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n",
        "                \n",
        "            else:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
        "            \n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
        "                \n",
        "                optimizer.step()\n",
        "                scheduler.step()  # Update learning rate schedule\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
        "                    # Log metrics\n",
        "                    if args['evaluate_during_training']:  # Only evaluate when single GPU otherwise metrics may not average well\n",
        "                        results = evaluate(model, tokenizer)\n",
        "                        \n",
        "\n",
        "                    logging_loss = tr_loss\n",
        "                    \n",
        "\n",
        "                if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args['output_dir'], 'checkpoint')\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
        "            \n",
        "\n",
        "    return global_step, tr_loss / global_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fXUP-xd8cqXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_runtime(data,gt,tokenizer,prefix=\"\"):\n",
        "    eval_dataset = load_and_cache_examples_runtime(data,gt, tokenizer, undersample_scale_factor=1, evaluate=True)\n",
        "\n",
        "    args=args_bert\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
        "\n",
        "    print(\"Run time evaluation on progress!!\")\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    out_label_ids = None\n",
        "    probs=None\n",
        "    net_input=None\n",
        "    featurelis=[]\n",
        "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "            tmp_eval_loss2, logits2 = outputs[:2]\n",
        "            \n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "        \n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            probs= torch.nn.functional.softmax(logits2.detach().cpu(), dim=1)\n",
        "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "            net_input=inputs['input_ids'].detach().cpu().numpy()\n",
        "            featurelis.append(model.bert(inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])[1].data.cpu().numpy())\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "            probs=np.append(probs,torch.nn.functional.softmax(logits2.detach().cpu(), dim=1), axis=0)\n",
        "            net_input=np.append(net_input,inputs['input_ids'].detach().cpu().numpy(),axis=0)\n",
        "            featurelis.append(model.bert(inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])[1].data.cpu().numpy())\n",
        "              \n",
        "\n",
        "    return probs,preds,featurelis#,out_label_ids,net_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tUvkEBZUz7Bk",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix, classification_report\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "def get_mismatched(labels, preds):\n",
        "    mismatched = labels != preds\n",
        "    examples = processor.get_dev_examples(args_bert['data_dir'])\n",
        "    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n",
        "    \n",
        "    return wrong\n",
        "\n",
        "def get_eval_report(labels, preds):\n",
        "    mcc = matthews_corrcoef(labels, preds)\n",
        "    print(classification_report(labels, preds))\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
        "    print(classification_report(labels, preds))\n",
        "   \n",
        "    return {\n",
        "        \"mcc\": mcc,\n",
        "        \"tp\": tp,\n",
        "        \"tn\": tn,\n",
        "        \"fp\": fp,\n",
        "        \"fn\": fn\n",
        "    }, get_mismatched(labels, preds)\n",
        "\n",
        "def compute_metrics(task_name, preds, labels):\n",
        "    assert len(preds) == len(labels)\n",
        "    return get_eval_report(labels, preds)\n",
        "\n",
        "def evaluate(model, argus, tokenizer, prefix=\"\",datatype=\"dev\"):\n",
        "    \n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    args=argus.copy()\n",
        "    eval_output_dir = args['output_dir']\n",
        "\n",
        "    results = {}\n",
        "    EVAL_TASK = args['task_name']\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(EVAL_TASK, tokenizer, args, undersample_scale_factor=1, evaluate=True,data_type=datatype)\n",
        "    if not os.path.exists(eval_output_dir):\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    #print(len(eval_dataset))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    out_label_ids = None\n",
        "    probs=None\n",
        "    net_input=None\n",
        "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            feature=model.bert(inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])[1].cpu().numpy()\n",
        "            print(feature.shape)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "        \n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            probs= preds.copy()\n",
        "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
        "            net_input=inputs['input_ids'].detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
        "            probs=np.append(probs,logits.detach().cpu().numpy(), axis=0)\n",
        "            net_input=np.append(net_input,inputs['input_ids'].detach().cpu().numpy(),axis=0)\n",
        "            \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    if args['output_mode'] == \"classification\":\n",
        "        preds = np.argmax(preds, axis=1)\n",
        "    elif args['output_mode'] == \"regression\":\n",
        "        preds = np.squeeze(preds)\n",
        "    result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
        "    results.update(result)\n",
        "\n",
        "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
        "    with open(output_eval_file, \"w\") as writer:\n",
        "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
        "        for key in sorted(result.keys()):\n",
        "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
        "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
        "\n",
        "    return results, wrong,probs,preds,out_label_ids,net_input\n",
        "\n",
        "\n",
        "def extract_features(model, argus, tokenizer, prefix=\"\",datatype=\"dev\"):\n",
        "    \n",
        "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
        "    args=argus.copy()\n",
        "    eval_output_dir = args['output_dir']\n",
        "\n",
        "    results = {}\n",
        "    EVAL_TASK = args['task_name']\n",
        "\n",
        "    eval_dataset = load_and_cache_examples(EVAL_TASK, tokenizer, args, undersample_scale_factor=1, evaluate=True,data_type=datatype)\n",
        "    if not os.path.exists(eval_output_dir):\n",
        "        os.makedirs(eval_output_dir)\n",
        "\n",
        "\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
        "\n",
        "    # Eval!\n",
        "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
        "    #print(len(eval_dataset))\n",
        "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
        "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
        "\n",
        "\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    out_label_ids = None\n",
        "    probs=None\n",
        "    net_input=None\n",
        "    \n",
        "    #save data\n",
        "    all_features=[]\n",
        "    all_sentences=[]\n",
        "    all_labels=[]\n",
        "\n",
        "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        # print(tokenizer.decode(batch[0])#.decode)\n",
        "        with torch.no_grad():\n",
        "            inputs = {'input_ids':      batch[0],\n",
        "                      'attention_mask': batch[1],\n",
        "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
        "                      'labels':         batch[3]}\n",
        "            outputs = model(**inputs)\n",
        "            feature=model.roberta(inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])[1].cpu().numpy()\n",
        "            \n",
        "            all_features.append(feature[0])\n",
        "            all_sentences.append(feature)\n",
        "            all_labels.append(batch[3].cpu().numpy())\n",
        "\n",
        "    return all_features,all_sentences,all_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MlaeXY9sz7Bm",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "#Train_bert\n",
        "if args_bert['do_train'] and False:\n",
        "    print(\"Training Bert!!\")\n",
        "    train_dataset = load_and_cache_examples(task, tokenizer_bert,args_bert,undersample_scale_factor=1)\n",
        "    #print(train_dataset)\n",
        "    global_step, tr_loss = train(train_dataset, model_bert, tokenizer_bert,args_bert)\n",
        "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1On6YjIULf7v",
        "scrolled": true,
        "outputId": "ac34320b-0732-4f5b-eded-6508f105925d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#Save bert\n",
        "if args_bert['do_train']:\n",
        "    if not os.path.exists(args_bert['output_dir']):\n",
        "            os.makedirs(args_bert['output_dir'])\n",
        "    logger.info(\"Saving model checkpoint to %s\", args_bert['output_dir'])\n",
        "    \n",
        "    model_to_save = model_bert.module if hasattr(model_bert, 'module') else model_bert  # Take care of distributed/parallel training\n",
        "    model_to_save.save_pretrained(args_bert['output_dir'])\n",
        "    tokenizer_bert.save_pretrained(args_bert['output_dir'])\n",
        "    torch.save(args_bert, os.path.join(args_bert['output_dir'], 'training_args.bin'))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Saving model checkpoint to outputs_robert-base-0.9_taskbc\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfKV6fwii3ev",
        "colab_type": "text"
      },
      "source": [
        "# Get test set predictions and probs for BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "nlIp1xZai3ex",
        "colab_type": "code",
        "outputId": "b651f057-e136-408b-e8b0-f0c3290bcb9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383,
          "referenced_widgets": [
            "fa8ded99567b4f45835ab6497de25ced",
            "d3b6a9386f0440cdb99371e69cc01b62",
            "b3c94b1750844b128b70dd2f4cc2a823",
            "04b35dd744f0406fb916dce91800e087",
            "d8a93f3059aa45f0865796d611353e67",
            "1593a9d326c9484d9d6af02e666a4983",
            "95028519633243b4bc54edd123e255d4",
            "6f1efde09769464dbfaaca20b286ceaf",
            "3c31864a936540b89f42db0dc21e794f",
            "0a40c124a6e849f9872bbe80dd2955d7",
            "7fec7b827818456d9b91ab6e47e6933a",
            "46b48d1afe37435188dc9e71d57ecd3e",
            "90e72cc301ef45d7a71fe132a3c52625",
            "53fb87ea5a4148cab0b5dbb5568c6c83",
            "a38c0448cff7455ba99170aae9d4d6c8",
            "8ee4922f7a95484f98d1d312873ff12d"
          ]
        }
      },
      "source": [
        "results = {}\n",
        "if args_roberta['do_eval']:\n",
        "    checkpoints = [args_roberta['output_dir']]\n",
        "    if args_roberta['eval_all_checkpoints']:\n",
        "        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args_roberta['output_dir']+ '/**/' + WEIGHTS_NAME, recursive=True)))\n",
        "        logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "    for checkpoint in checkpoints:\n",
        "        print(checkpoint)\n",
        "        if True:\n",
        "            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
        "            model = model_class_roberta.from_pretrained(checkpoint)\n",
        "            model.to(device)\n",
        "            xtest, sentence,ytest= extract_features(model, args_roberta,tokenizer_roberta, prefix=global_step,datatype=\"test\")\n",
        "\n",
        "\n",
        "results = {}\n",
        "if args_roberta['do_eval']:\n",
        "    checkpoints = [args_roberta['output_dir']]\n",
        "    if args_roberta['eval_all_checkpoints']:\n",
        "        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args_roberta['output_dir']+ '/**/' + WEIGHTS_NAME, recursive=True)))\n",
        "        logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "    for checkpoint in checkpoints:\n",
        "        print(checkpoint)\n",
        "        if True:\n",
        "            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
        "            model = model_class_roberta.from_pretrained(checkpoint)\n",
        "            model.to(device)\n",
        "            xtrain, sentence,ytrain= extract_features(model, args_roberta,tokenizer_roberta, prefix=global_step,datatype=\"train\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Evaluate the following checkpoints: ['outputs_robert-base-0.9_taskbc']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "outputs_robert-base-0.9_taskbc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 184/184 [00:00<00:00, 614.36it/s]\n",
            "INFO:__main__:Saving features into cached file data_new_1/cached_dev_roberta-base_128_binary\n",
            "INFO:__main__:***** Running evaluation  *****\n",
            "INFO:__main__:  Num examples = 184\n",
            "INFO:__main__:  Batch size = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa8ded99567b4f45835ab6497de25ced",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=184, style=ProgressStyle(description_width='â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Evaluate the following checkpoints: ['outputs_robert-base-0.9_taskbc']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "outputs_robert-base-0.9_taskbc\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 276/276 [00:00<00:00, 812.34it/s]\n",
            "INFO:__main__:Saving features into cached file data_new_1/cached_dev_roberta-base_128_binary\n",
            "INFO:__main__:***** Running evaluation  *****\n",
            "INFO:__main__:  Num examples = 276\n",
            "INFO:__main__:  Batch size = 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c31864a936540b89f42db0dc21e794f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=276, style=ProgressStyle(description_width='â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwhP1OWpep64",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#Runtime execution based ideas\n",
        "def load_and_cache_examples_runtime(data,gt,tokenizer, evaluate=False, undersample_scale_factor=1,task=\"binary\"):\n",
        "    processor = processors[task]()\n",
        "    output_mode = args_bert['output_mode']\n",
        "    args=args_bert\n",
        "    \n",
        "    examples_new = []\n",
        "    count=0\n",
        "    for  line,lab in zip(data,gt):\n",
        "            guid = \"%s-%s\" % (\"test\", count)\n",
        "            text_a = line\n",
        "            label = lab\n",
        "            #print(text_a,label)\n",
        "            examples_new.append(\n",
        "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "            count+=1\n",
        "    \n",
        "    if True:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n",
        "        label_list = processor.get_labels()\n",
        "        examples = examples_new#processor.get_dev_examples(args['data_dir']) if evaluate else processor.get_train_examples(args['data_dir'])\n",
        "#         print(len(examples))\n",
        "        examples  = [example for example in examples]\n",
        "        #print((examples))\n",
        "        \n",
        "        features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer, output_mode,\n",
        "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),            # xlnet has a cls token at the end\n",
        "            cls_token=tokenizer.cls_token,\n",
        "            sep_token=tokenizer.sep_token,\n",
        "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
        "            pad_on_left=bool(args['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n",
        "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0,\n",
        "            process_count=2)\n",
        "        \n",
        "       \n",
        "        \n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
        "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
        "    if output_mode == \"classification\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
        "    elif output_mode == \"regression\":\n",
        "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9z_EIFKe0KsN",
        "colab_type": "code",
        "outputId": "0d7badec-4f19-46e1-fc19-2ceff57239b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 490
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn.model_selection import RandomizedSearchCV,StratifiedKFold\n",
        "from sklearn.dummy import DummyClassifier\n",
        "import pickle\n",
        "\n",
        "\n",
        "params = {\n",
        "        'min_child_weight': [1, 5, 10],\n",
        "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
        "        'subsample': [0.6, 0.8, 1.0],\n",
        "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
        "        'max_depth': [3, 4, 5]\n",
        "        }\n",
        "\n",
        "ytrain=np.array(ytrain).ravel()\n",
        "ytest=np.array(ytest).ravel()\n",
        "\n",
        "\n",
        "class_weights = list(class_weight.compute_class_weight('balanced',np.unique(ytrain),np.array(ytrain)))\n",
        "w_array = np.ones(ytrain.shape[0], dtype = 'float')\n",
        "\n",
        "for i, val in enumerate(ytrain):\n",
        "    w_array[i] = class_weights[val-1]\n",
        "\n",
        "clf= SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=100, random_state=42,class_weight=\"balanced\",warm_start=True)\n",
        "clf2=XGBClassifier(n_estimators=200,random_state=10,max_depth=3,learning_rate =0.1)\n",
        "\n",
        "clf.fit(np.array(xtrain),np.array(ytrain).astype(np.int32))\n",
        "clf2.fit(np.array(xtrain),np.array(ytrain).astype(np.int32))\n",
        "\n",
        "ypred=clf.predict(xtest)\n",
        "ypred2=clf2.predict(xtest)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"SGD\")\n",
        "print(classification_report(np.array(ytest).ravel(),ypred,digits=5))\n",
        "\n",
        "\n",
        "print(\"XGB\")\n",
        "print(classification_report(np.array(ytest).ravel(),ypred2,digits=5))\n",
        "\n",
        "if True:\n",
        "  files = open('xtrain.pkl', 'wb')\n",
        "\n",
        "  # dump information to that file\n",
        "  pickle.dump(xtrain, files)\n",
        "  files = open('ytrain.pkl', 'wb')\n",
        "\n",
        "  # dump information to that file\n",
        "  pickle.dump(ytrain, files)\n",
        "\n",
        "  files = open('xtest.pkl', 'wb')\n",
        "\n",
        "  # dump information to that file\n",
        "  pickle.dump(xtest, files)\n",
        "\n",
        "  files = open('ytest.pkl', 'wb')\n",
        "\n",
        "  # dump information to that file\n",
        "  pickle.dump(ytest, files)\n",
        "  files.close()\n",
        "  print(np.array(xtrain).shape)\n",
        "\n",
        "#change classifier here\n",
        "clfimp=clf2"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SGD\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.00000   0.00000   0.00000        58\n",
            "           1    0.00000   0.00000   0.00000        90\n",
            "           2    0.00000   0.00000   0.00000        12\n",
            "           3    0.13043   1.00000   0.23077        24\n",
            "\n",
            "    accuracy                        0.13043       184\n",
            "   macro avg    0.03261   0.25000   0.05769       184\n",
            "weighted avg    0.01701   0.13043   0.03010       184\n",
            "\n",
            "XGB\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0    0.73333   0.37931   0.50000        58\n",
            "           1    0.55944   0.88889   0.68670        90\n",
            "           2    0.25000   0.08333   0.12500        12\n",
            "           3    0.42857   0.12500   0.19355        24\n",
            "\n",
            "    accuracy                        0.57609       184\n",
            "   macro avg    0.49284   0.36913   0.37631       184\n",
            "weighted avg    0.57700   0.57609   0.52689       184\n",
            "\n",
            "(276, 768)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H0NViOMU-alD",
        "colab_type": "code",
        "outputId": "da82a7fe-eb2c-4b5f-e985-c82f85450174",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!mkdir set2_bert_inter"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory â€˜set2_bert_interâ€™: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_HWT93QdCJ4",
        "colab_type": "text"
      },
      "source": [
        "BERT LIME INTERPRETABILITY TEST"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuniKq3GdFmw",
        "colab_type": "code",
        "outputId": "02fbd37a-44cd-4498-98f0-f88d24329d10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "65eb1e5ce3ff46ce92d59acbd4401038",
            "4073e7999dc84c1d94123e9ebb8d0661",
            "ecfa453c6624458b8890d1294d05c4f9",
            "8573bc87c5824f4db76393ba8ae370df",
            "af5c6f64f77b4056a76dd55826d0286b",
            "6b5dc9903d6f4e93938055c356fc1933",
            "62b702d4b3ac4ac8ad6c926413479ea3",
            "c401bed5ca924e5f8d7895368a59421f",
            "aa58a1aed9d343cf83426a43acbad429",
            "b493bd3b86864cd6903bf619af99db5f",
            "bb78420a5da6460eb5c36ff6565ddeb9",
            "735f045a9c6141e6a3318f7de97db6b6",
            "6a2f92eb6b884c489da30649b0224088",
            "92d84da16fdc4da390af48ce759b7a9d",
            "1ea64c5088454160966ed56475aab685",
            "9a1d30b493394fb6917e0d667d026b47",
            "73695784105646cf8c2dfe039512e1cd",
            "17e2e02d669245b7aaaa681541a33f86",
            "6917e1fad67c4dfd9bd129dd8b4af719",
            "388c374130a74295a44b1140760617c2",
            "9f46140e2fd94288a91ed453dafb1e53",
            "1f655703beaf4912a46939220076ad22",
            "11de8d0b4ad94933bd9eba6138c922a6",
            "4405e5aa9c604c2e8bd38cab6d9a7761",
            "4a7e84042d8441e58f0c73903b137b2a",
            "e232f746ee3440daaffb6204f1c0fcd7",
            "52c569db0db74fe79ff6a981b021b3d2"
          ]
        }
      },
      "source": [
        "def get_probs_fts(features_bert):\n",
        "    global clfimp\n",
        "    return clfimp.predict_proba(np.array(features_bert))\n",
        "\n",
        "def callme(a,b=None):\n",
        "    proboab,preds,net_inputs=evaluate_runtime(a,[str(1) for i in a],tokenizer_bert,prefix=\"\")\n",
        "    net_inputs=np.array(net_inputs)\n",
        "    net_inputs=net_inputs.squeeze()\n",
        "    print(net_inputs.shape)\n",
        "    probs_new=clfimp.predict_proba(net_inputs)\n",
        "    return probs_new\n",
        "            \n",
        "    \n",
        "from lime import lime_text\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "explainer = LimeTextExplainer(class_names=[\"0\",\"1\",\"2\",\"3\"])\n",
        "index=0\n",
        "for txt in test_df[\"text\"]:\n",
        "    print(txt)\n",
        "    exp = explainer.explain_instance(txt, callme, num_features=128,top_labels=4)\n",
        "    #exp.show_in_notebook(text=True)\n",
        "    exp.save_to_file(\"set2_bert_inter/\"+\"results_\"+str(index)+\".html\")\n",
        "    index+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "This agent still canâ€™t handle problems involving shapes which have shading.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2522.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65eb1e5ce3ff46ce92d59acbd4401038",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "This agent revision has improved performance across all E sets, as this union algorithm is both simple and fairly general.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2401.19it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4073e7999dc84c1d94123e9ebb8d0661",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "The Dark Pixel Ratio methodmust be fine-tuned even more to solve the other problems.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2707.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ecfa453c6624458b8890d1294d05c4f9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Other than that, the root mean square difference is alsoused in certain places.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2761.41it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8573bc87c5824f4db76393ba8ae370df",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            ", this is unlike human behaviour which appearsto be logical but doesnâ€™t use any logic as part of the reasoning strategy.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2314.73it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "af5c6f64f77b4056a76dd55826d0286b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "I was pleasantly surprised by the agents ability to transfer much of its capability from the previous problem sets to these new ones.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2377.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b5dc9903d6f4e93938055c356fc1933",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Test and Raven set show no improvement.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 3179.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "62b702d4b3ac4ac8ad6c926413479ea3",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Additionally, linear patterns between constraints can be identified as well.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2810.71it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c401bed5ca924e5f8d7895368a59421f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "I also removed the ability to transform (translate) shapes, which was somewhat inefficient.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2292.02it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa58a1aed9d343cf83426a43acbad429",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Also the rules were re-sorted such thatrules with hard thresholds for RMS and ED were given priority in processing.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2501.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b493bd3b86864cd6903bf619af99db5f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "It is only able to identify the pattern that ispresent horizontally.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2948.68it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb78420a5da6460eb5c36ff6565ddeb9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "It pre-processes each of the lettered images independently, so it runs in linear time, where n is the lettered images.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2383.48it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "735f045a9c6141e6a3318f7de97db6b6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "The remaining set D and E were unique and I couldnâ€™t find how to solve them.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2738.55it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6a2f92eb6b884c489da30649b0224088",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Computationally, theagentâ€™s cognition is similar to the previous submission.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2638.88it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92d84da16fdc4da390af48ce759b7a9d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "The agent solves CP-6 and CP-7, resulting in improvement on challenge set, however the test set performance remained same.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2323.82it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ea64c5088454160966ed56475aab685",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "It is also not able to adapt to new problem domains, by transferring knowledge from previous domains.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2543.83it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9a1d30b493394fb6917e0d667d026b47",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Unchanged transformations only check for images to beidentical and are also similar to human cognition.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2611.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "73695784105646cf8c2dfe039512e1cd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "The limitations of the agent could be improved considerably by adding new rules to tackle skipped problems and solving CP-4 througha different solution logic.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2144.69it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "17e2e02d669245b7aaaa681541a33f86",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "The agent cannot still track thetransformation involving overlapping of two images.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2705.29it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6917e1fad67c4dfd9bd129dd8b4af719",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Once the ratio was established,finding the answer was a question of setting the ratio with the most scarce difference.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2397.34it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "388c374130a74295a44b1140760617c2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "As such rule-11 was modified (red highlighted) to accommodate this change, resulting in rule 14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2605.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f46140e2fd94288a91ed453dafb1e53",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "When compared to the highest score achieved till this submission, it solves oneproblem extra in Ravens problems in Set E and 9 problems in Set D Basic, TestRaven and Challenge together.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 1890.17it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1f655703beaf4912a46939220076ad22",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "For BP-2, the affine relationship is not straightforward.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2945.98it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "11de8d0b4ad94933bd9eba6138c922a6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Analysis revealed thatmost parts of the rules are valid except for comparison between A and D. Previously in problem BP-10, B and D were same, where as in CP-3, this is not trueleading to failure of the comparison.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 1838.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4405e5aa9c604c2e8bd38cab6d9a7761",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Moreover,the agent is now a matured production system for solving BPâ€™s with productionrules in long term memory for solving RPMâ€™s.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2311.10it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4a7e84042d8441e58f0c73903b137b2a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "It uses the AND operation on the image arrays tofind out the overlapped image.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:01<00:00, 2878.80it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e232f746ee3440daaffb6204f1c0fcd7",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "(5000, 768)\n",
            "Most importantly the performance on test set and ravens setsimproved to 6/12 and the developed rule also solved BP-12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
            "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
            "INFO:__main__:Creating features from dataset file at data_new_1/\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5000/5000 [00:02<00:00, 2446.66it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Run time evaluation on progress!!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "52c569db0db74fe79ff6a981b021b3d2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=â€¦"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Buffered data was truncated after reaching the output size limit."
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}