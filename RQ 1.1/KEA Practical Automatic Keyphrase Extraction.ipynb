{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TERM FREQUENCY COUNTER BASED ON N-GRAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from string import punctuation\n",
    "\n",
    "from pke import compute_document_frequency\n",
    "\n",
    "# setting info in terminal\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# path to the collection of documents\n",
    "input_dir = './train_data/document/test/'\n",
    "\n",
    "# path to the df weights dictionary, saved as a gzipped csv file\n",
    "output_file = \"df_kea_test.tsv.gz\"\n",
    "\n",
    "# stoplist are punctuation marks\n",
    "stoplist = list(punctuation)\n",
    "stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "\n",
    "# compute idf weights\n",
    "compute_document_frequency(input_dir=input_dir,\n",
    "                           output_file=output_file,\n",
    "                           extension='txt', # input file extension\n",
    "                           language='en', # language of the input files\n",
    "                           normalization=\"stemming\", # use porter stemmer\n",
    "                           stoplist=stoplist,  # stoplist\n",
    "                           delimiter='\\t',  # tab separated output\n",
    "                           n=20)  # compute n-grams up to 5-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING KEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#trainining\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pke\n",
    "\n",
    "# setting info in terminal\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# path to the collection of documents\n",
    "input_dir = './train_data/document/train/'\n",
    "\n",
    "# path to the reference file\n",
    "reference_file = \"./train_data/gold-annotation/train_gold.txt\"\n",
    "\n",
    "# path to the df file\n",
    "df_file = \"df_kea_train.tsv.gz\"\n",
    "logging.info('Loading df counts from {}'.format(df_file))\n",
    "df_counts = pke.load_document_frequency_file(input_file=df_file,\n",
    "                                             delimiter='\\t')\n",
    "\n",
    "# path to the model, saved as a pickle\n",
    "output_mdl = \"kea-model.pickle\"\n",
    "\n",
    "pke.train_supervised_model(input_dir=input_dir,\n",
    "                           reference_file=reference_file,\n",
    "                           model_file=output_mdl,\n",
    "                           extension='txt',\n",
    "                           language='en',\n",
    "                           normalization=\"stemming\",\n",
    "                           df=df_counts,\n",
    "                           model=pke.supervised.Kea())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING  WINGUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#trainining\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pke\n",
    "\n",
    "# setting info in terminal\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# path to the collection of documents\n",
    "input_dir = './train_data/document/train/'\n",
    "\n",
    "# path to the reference file\n",
    "reference_file = \"./train_data/gold-annotation/train_gold.txt\"\n",
    "\n",
    "# path to the df file\n",
    "df_file = \"df_kea_train.tsv.gz\"\n",
    "logging.info('Loading df counts from {}'.format(df_file))\n",
    "df_counts = pke.load_document_frequency_file(input_file=df_file,\n",
    "                                             delimiter='\\t')\n",
    "\n",
    "# path to the model, saved as a pickle\n",
    "output_mdl = \"kea-model.pickle\"\n",
    "\n",
    "pke.train_supervised_model(input_dir=input_dir,\n",
    "                           reference_file=reference_file,\n",
    "                           model_file=output_mdl,\n",
    "                           extension='txt',\n",
    "                           language='en',\n",
    "                           normalization=\"stemming\",\n",
    "                           df=df_counts,\n",
    "                           model=pke.supervised.WINGUS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "from collections import Counter\n",
    "# create a Kea extractor and set the input language to English (used for\n",
    "# the stoplist in the candidate selection method)\n",
    "extractor = pke.supervised.Kea()\n",
    "#extractor = pke.supervised.WINGUS()\n",
    "\n",
    "# load the content of the document, here in CoreNLP XML format\n",
    "# the use_lemmas parameter allows to choose using CoreNLP lemmas or stems \n",
    "# computed using nltk\n",
    "extractor.load_document('./train_data/document/train/train.txt')\n",
    "\n",
    "# select the keyphrase candidates, for Kea the 1-3 grams that do not start or\n",
    "# end with a stopword.\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# load the df counts\n",
    "df_counts = pke.load_document_frequency_file(input_file=\"df_kea_train.tsv.gz\",\n",
    "                                             delimiter='\\t')\n",
    "\n",
    "# weight the candidates using Kea model.\n",
    "extractor.candidate_weighting(model_file=\"kea-model.pickle\", df=df_counts)\n",
    "\n",
    "# print the n-highest (10) scored candidates\n",
    "allkeyphrases=[]\n",
    "# print the n-highest (10) scored candidates\n",
    "for (keyphrase, score) in extractor.get_n_best(n=10, stemming=False):\n",
    "    allkeyphrases.append(keyphrase)\n",
    "\n",
    "df=pd.read_csv(\"./train_data/tsv/test2.tsv\",delimiter=\"\\t\")\n",
    "texts=df[\"text\"]\n",
    "texts=[i.replace(\",\",\"\") for i in texts]\n",
    "labels=df[\"label\"]\n",
    "labels=[0 if i==0 else 1 for i in labels]\n",
    "\n",
    "overall_evidence=[]\n",
    "removed_text=[]\n",
    "for txt in texts:\n",
    "    evidence=[]\n",
    "    for phr in allkeyphrases :\n",
    "        #print(phr,\"--\",txt, phr in txt)\n",
    "        if phr in txt and txt not in removed_text:\n",
    "            evidence.append(1)\n",
    "            removed_text.append(txt)\n",
    "    if evidence==[]:\n",
    "        evidence=[0]\n",
    "    overall_evidence.append(evidence)\n",
    "\n",
    "ypred=[Counter(i).most_common(1)[0][0] for i in overall_evidence]\n",
    "print(ypred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels,ypred,digits=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEA Implementation with SVM rather than Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously we used KEA that originally is implemented based on https://arxiv.org/abs/cs/9902007. Here we instead reimpliment the same with SVM based model rather than originally used Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "traindata=pd.read_csv(\"./train_data/tsv/test1.tsv\",delimiter=\"\\t\")\n",
    "X_test=traindata.text.values\n",
    "Y_test=np.array(traindata.label.values).astype(np.int32)\n",
    "\n",
    "traindata=pd.read_csv(\"./train_data/tsv/train1.tsv\",delimiter=\"\\t\")\n",
    "X_train=traindata.text.values\n",
    "Y_train=np.array(traindata.label.values).astype(np.int32)\n",
    "clf = Pipeline([('vect', CountVectorizer(ngram_range=(1,2))), ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                         ('clf', SGDClassifier(loss='log', penalty='l2',alpha=0.001, max_iter=100, random_state=42,class_weight=\"balanced\",warm_start=True))])\n",
    "\n",
    "\n",
    "Y_test=[0 if i==0 else 1 for i in Y_test]\n",
    "Y_train=[0 if i==0 else 1 for i in Y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train,Y_train)\n",
    "ypred=clf.predict(X_test)\n",
    "\n",
    "print(classification_report(Y_test,ypred,digits=5))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
