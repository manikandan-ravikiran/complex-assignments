{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0YLoS0hWz-ch",
    "outputId": "9bd4de3e-84e1-4253-d4aa-61b6fd77491e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.4.0)\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.5.0)\n",
      "Collecting pytorch-transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/b7/d3d18008a67e0b968d1ab93ad444fc05699403fa662f634b2f2c318a508b/pytorch_transformers-1.2.0-py3-none-any.whl (176kB)\n",
      "\r",
      "\u001b[K     |█▉                              | 10kB 24.4MB/s eta 0:00:01\r",
      "\u001b[K     |███▊                            | 20kB 29.5MB/s eta 0:00:01\r",
      "\u001b[K     |█████▋                          | 30kB 32.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 40kB 35.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▎                      | 51kB 36.2MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▏                    | 61kB 38.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████                   | 71kB 39.4MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 81kB 40.4MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▊               | 92kB 42.2MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▋             | 102kB 43.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▍           | 112kB 43.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▎         | 122kB 43.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 133kB 43.9MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 143kB 43.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▉    | 153kB 43.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 163kB 43.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 174kB 43.9MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 184kB 43.9MB/s \n",
      "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.12.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (6.2.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.17.5)\n",
      "Collecting sentencepiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
      "\u001b[K     |████████████████████████████████| 1.0MB 18.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (4.28.1)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/b4/7a41d630547a4afd58143597d5a49e07bfd4c42914d8335b2a5657efc14b/sacremoses-0.0.38.tar.gz (860kB)\n",
      "\u001b[K     |████████████████████████████████| 870kB 38.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2.21.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (1.11.15)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-transformers) (2019.12.20)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch-transformers) (0.14.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (1.24.3)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-transformers) (3.0.4)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.9.4)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-transformers) (1.14.15)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch-transformers) (2.6.1)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.38-cp36-none-any.whl size=884628 sha256=0ad49dfd063e73d395a0a2bcc7d44286af111ef0cd07b3ad24a06334bc74d4c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/6d/ec/1a/21b8912e35e02741306f35f66c785f3afe94de754a0eaf1422\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sentencepiece, sacremoses, pytorch-transformers\n",
      "Successfully installed pytorch-transformers-1.2.0 sacremoses-0.0.38 sentencepiece-0.1.85\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (0.22.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
      "Collecting lime\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b5/e0/60070b461a589b2fee0dbc45df9987f150fca83667c2f8a064cef7dbac6b/lime-0.1.1.37.tar.gz (275kB)\n",
      "\u001b[K     |████████████████████████████████| 276kB 45.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.17.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
      "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.1)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.27.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
      "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.1)\n",
      "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.9.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.8)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
      "Collecting progressbar\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/a6/b8e451f6cff1c99b4747a2f7235aa904d2d49e8e1464e0b798272aa84358/progressbar-2.5.tar.gz\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from lime) (3.1.3)\n",
      "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.6/dist-packages (from lime) (0.16.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (45.2.0)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow) (2.8.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (3.2.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (2.4.6)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->lime) (0.10.0)\n",
      "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (6.2.2)\n",
      "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4.1)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (2.4)\n",
      "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.12->lime) (1.1.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.12->lime) (4.4.1)\n",
      "Building wheels for collected packages: lime, progressbar\n",
      "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for lime: filename=lime-0.1.1.37-cp36-none-any.whl size=284277 sha256=1fb425a97ad3c20d98ff2076f8a4f881317eaf7c5cb183f8527f20524f3204a3\n",
      "  Stored in directory: /root/.cache/pip/wheels/c1/38/e7/50d75d4fb75afa604570dc42f20c5c5f5ab26d3fbe8d6ef27b\n",
      "  Building wheel for progressbar (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for progressbar: filename=progressbar-2.5-cp36-none-any.whl size=12074 sha256=8d278c35f5253f1f027f20387c68ba44b99ccf5ce3c3a141673eab1a0a03c88f\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/e9/6b/ea01090205e285175842339aa3b491adeb4015206cda272ff0\n",
      "Successfully built lime progressbar\n",
      "Installing collected packages: progressbar, lime\n",
      "Successfully installed lime-0.1.1.37 progressbar-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install torch torchvision pytorch-transformers\n",
    "!pip install scikit-learn pandas tensorflow lime\n",
    "\n",
    "from __future__ import absolute_import, division, print_function\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(0)\n",
    "import csv\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "from io import open\n",
    "\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score\n",
    "\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from tqdm import tqdm\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "csv.field_size_limit(2147483647)\n",
    "# writerobj = SummaryWriter()\n",
    "\n",
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, label=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            label: (Optional) string. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.label = label\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_id):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_id = label_id\n",
    "\n",
    "\n",
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    @classmethod\n",
    "    def _read_tsv(cls, input_file, quotechar=None):\n",
    "        \"\"\"Reads a tab separated value file.\"\"\"\n",
    "        with open(input_file, \"r\", encoding=\"utf-8-sig\") as f:\n",
    "            reader = csv.reader(f, delimiter=\"\\t\", quotechar=quotechar)\n",
    "            lines = []\n",
    "            for line in reader:\n",
    "                if sys.version_info[0] == 2:\n",
    "                    line = list(unicode(cell, 'utf-8') for cell in line)\n",
    "                lines.append(line)\n",
    "            return lines\n",
    "\n",
    "\n",
    "class BinaryProcessor(DataProcessor):\n",
    "    \"\"\"Processor for the binary data sets\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\")\n",
    "\n",
    "    def get_dev_examples(self, data_dir,data_type=\"dev\"):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return self._create_examples(\n",
    "            self._read_tsv(os.path.join(data_dir, data_type+\".tsv\")), \"dev\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        return [\"0\",\"1\",\"2\",\"3\"]\n",
    "\n",
    "    def _create_examples(self, lines, set_type):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        examples = []\n",
    "        for (i, line) in enumerate(lines):\n",
    "            \n",
    "            guid = \"%s-%s\" % (set_type, i)\n",
    "            try:\n",
    "                text_a = line[3]\n",
    "                label = line[1]\n",
    "            except:\n",
    "                print(line)\n",
    "            #print(text_a,label)\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "        return examples\n",
    "\n",
    "\n",
    "def convert_example_to_feature(example_row, pad_token=0,\n",
    "sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "cls_token_segment_id=1, pad_token_segment_id=0,\n",
    "mask_padding_with_zero=True):\n",
    "    \n",
    "   \n",
    "    \n",
    "    example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id = example_row\n",
    "    #print(example.text_a,\"label-------->\",example.label)\n",
    "    tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "    tokens_b = None\n",
    "    if example.text_b:\n",
    "        tokens_b = tokenizer.tokenize(example.text_b)\n",
    "        # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "        # length is less than the specified length.\n",
    "        # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "        _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "    else:\n",
    "        # Account for [CLS] and [SEP] with \"- 2\"\n",
    "        if len(tokens_a) > max_seq_length - 2:\n",
    "            tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "    # The convention in BERT is:\n",
    "    # (a) For sequence pairs:\n",
    "    #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "    #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "    # (b) For single sequences:\n",
    "    #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "    #  type_ids:   0   0   0   0  0     0   0\n",
    "    #\n",
    "    # Where \"type_ids\" are used to indicate whether this is the first\n",
    "    # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "    # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "    # embedding vector (and position vector). This is not *strictly* necessary\n",
    "    # since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "    # it easier for the model to learn the concept of sequences.\n",
    "    #\n",
    "    # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "    # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "    # the entire model is fine-tuned.\n",
    "    tokens = tokens_a + [sep_token]\n",
    "    segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "    if tokens_b:\n",
    "        tokens += tokens_b + [sep_token]\n",
    "        segment_ids += [sequence_b_segment_id] * (len(tokens_b) + 1)\n",
    "\n",
    "    if cls_token_at_end:\n",
    "        tokens = tokens + [cls_token]\n",
    "        segment_ids = segment_ids + [cls_token_segment_id]\n",
    "    else:\n",
    "        tokens = [cls_token] + tokens\n",
    "        segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length.\n",
    "    padding_length = max_seq_length - len(input_ids)\n",
    "    if pad_on_left:\n",
    "        input_ids = ([pad_token] * padding_length) + input_ids\n",
    "        input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "        segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "    else:\n",
    "        input_ids = input_ids + ([pad_token] * padding_length)\n",
    "        input_mask = input_mask + ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "        segment_ids = segment_ids + ([pad_token_segment_id] * padding_length)\n",
    "\n",
    "    assert len(input_ids) == max_seq_length\n",
    "    assert len(input_mask) == max_seq_length\n",
    "    assert len(segment_ids) == max_seq_length\n",
    "    \n",
    "    #print(label_map)\n",
    "\n",
    "    if output_mode == \"classification\":\n",
    "        label_id = label_map[example.label]\n",
    "    elif output_mode == \"regression\":\n",
    "        label_id = float(example.label)\n",
    "    else:\n",
    "        raise KeyError(output_mode)\n",
    "\n",
    "    return InputFeatures(input_ids=input_ids,\n",
    "                        input_mask=input_mask,\n",
    "                        segment_ids=segment_ids,\n",
    "                        label_id=label_id)\n",
    "    \n",
    "\n",
    "def convert_examples_to_features(examples, label_list, max_seq_length,\n",
    "                                 tokenizer, output_mode,\n",
    "                                 cls_token_at_end=False, pad_on_left=False,\n",
    "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
    "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=1, pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True,\n",
    "                                 process_count=cpu_count() - 2):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "    #print(label_map)\n",
    "\n",
    "    examples = [(example, label_map, max_seq_length, tokenizer, output_mode, cls_token_at_end, cls_token, sep_token, cls_token_segment_id, pad_on_left, pad_token_segment_id) for example in examples]\n",
    "\n",
    "    with Pool(process_count) as p:\n",
    "        features = list(tqdm(p.imap(convert_example_to_feature, examples, chunksize=100), total=len(examples)))\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()\n",
    "\n",
    "\n",
    "processors = {\n",
    "    \"binary\": BinaryProcessor\n",
    "}\n",
    "\n",
    "output_modes = {\n",
    "    \"binary\": \"classification\"\n",
    "}\n",
    "\n",
    "GLUE_TASKS_NUM_LABELS = {\n",
    "    \"binary\": 4\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVJDURgfcGf2"
   },
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "qYv4tGNs4Tg8",
    "outputId": "6d98fc05-8416-46c6-acb4-54e2fb0b1df9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
      "\n",
      "Enter your authorization code:\n",
      "··········\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "import io\n",
    "from google.colab import files\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# trainfile = files.upload()\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "be9yhGBXkit1"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"/content/drive/My Drive/exp1/train2.tsv\",delimiter=\"\\t\")\n",
    "test_df = pd.read_csv(\"/content/drive/My Drive/exp1/test2.tsv\",delimiter=\"\\t\")\n",
    "dev_df = test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 402
    },
    "colab_type": "code",
    "id": "0mmUcSnZi3cv",
    "outputId": "a6e433b1-4821-4786-8b50-ddb2adff4411",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I next attempted to make the agent more genera...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The ability to recognize shapes, and detect th...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This agent still can’t solve problems which re...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>For Basic Problem D-06, not only does the shap...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In that sense, for buildingunique solution to ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>Analysis of Generalization: The rules for CP’s...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>C-06, D-05 uses C-09, Challenge D-01 uses C-12...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>For CP-3 again there is increasing dark pixels...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>For both problems the XOR constraints hold ver...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>Further, investigation on skipped problems rev...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>276 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  text  label\n",
       "0    I next attempted to make the agent more genera...      1\n",
       "1    The ability to recognize shapes, and detect th...      2\n",
       "2    This agent still can’t solve problems which re...      2\n",
       "3    For Basic Problem D-06, not only does the shap...      1\n",
       "4    In that sense, for buildingunique solution to ...      1\n",
       "..                                                 ...    ...\n",
       "271  Analysis of Generalization: The rules for CP’s...      2\n",
       "272  C-06, D-05 uses C-09, Challenge D-01 uses C-12...      1\n",
       "273  For CP-3 again there is increasing dark pixels...      4\n",
       "274  For both problems the XOR constraints hold ver...      4\n",
       "275  Further, investigation on skipped problems rev...      3\n",
       "\n",
       "[276 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "hVsnPNzr4XjB",
    "outputId": "817c265c-0309-4e77-ac28-96bf765ea400",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>I next attempted to make the agent more genera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>The ability to recognize shapes, and detect th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>This agent still can’t solve problems which re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>For Basic Problem D-06, not only does the shap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>In that sense, for buildingunique solution to ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      0     a  I next attempted to make the agent more genera...\n",
       "1   1      1     a  The ability to recognize shapes, and detect th...\n",
       "2   2      1     a  This agent still can’t solve problems which re...\n",
       "3   3      0     a  For Basic Problem D-06, not only does the shap...\n",
       "4   4      0     a  In that sense, for buildingunique solution to ..."
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.DataFrame({\n",
    "    'id':range(len(train_df)),\n",
    "    'label':train_df[\"label\"].astype(int)-1,\n",
    "    'alpha':['a']*train_df.shape[0],\n",
    "    'text': train_df[\"text\"].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "IkUFgZeh4Xf2",
    "outputId": "f3ee25b1-dbaf-4e40-bf80-bab449ede5f6",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>The performance of the agent has increased a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>This agent still can’t handle problems involvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>This agent revision has improved performance a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>The Dark Pixel Ratio methodmust be fine-tuned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Other than that, the root mean square differen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      1     a  The performance of the agent has increased a l...\n",
       "1   1      1     a  This agent still can’t handle problems involvi...\n",
       "2   2      1     a  This agent revision has improved performance a...\n",
       "3   3      1     a  The Dark Pixel Ratio methodmust be fine-tuned ...\n",
       "4   4      0     a  Other than that, the root mean square differen..."
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_df = pd.DataFrame({\n",
    "    'id':range(len(dev_df)),\n",
    "    'label':dev_df[\"label\"].astype(int)-1,\n",
    "    'alpha':['a']*dev_df.shape[0],\n",
    "    'text': dev_df[\"text\"].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "dev_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "rhQDq4-Gi3dJ",
    "outputId": "7d47f140-fa98-45b8-d24b-9701c6baf380",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>alpha</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>The performance of the agent has increased a l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>This agent still can’t handle problems involvi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>This agent revision has improved performance a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>The Dark Pixel Ratio methodmust be fine-tuned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>a</td>\n",
       "      <td>Other than that, the root mean square differen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label alpha                                               text\n",
       "0   0      1     a  The performance of the agent has increased a l...\n",
       "1   1      1     a  This agent still can’t handle problems involvi...\n",
       "2   2      1     a  This agent revision has improved performance a...\n",
       "3   3      1     a  The Dark Pixel Ratio methodmust be fine-tuned ...\n",
       "4   4      0     a  Other than that, the root mean square differen..."
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.DataFrame({\n",
    "    'id':range(len(test_df)),\n",
    "     'label':test_df[\"label\"].astype(int)-1,\n",
    "    'alpha':['a']*test_df.shape[0],\n",
    "    'text': test_df[\"text\"].replace(r'\\n', ' ', regex=True)\n",
    "})\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pFlvoH234XdO",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!mkdir data_new_1\n",
    "train_df.to_csv('data_new_1/train.tsv', sep='\\t', index=False, header=False, columns=train_df.columns)\n",
    "dev_df.to_csv('data_new_1/dev.tsv', sep='\\t', index=False, header=False, columns=dev_df.columns)\n",
    "test_df.to_csv('data_new_1/test.tsv', sep='\\t', index=False, header=False, columns=test_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "VeXuXWylz7BD",
    "outputId": "ecedf77c-725b-4b82-d048-f98556911ece",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pytorch_transformers in /usr/local/lib/python3.6/dist-packages (1.2.0)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.4.0)\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2019.12.20)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (4.28.1)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.1.85)\n",
      "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (0.0.38)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.17.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (2.21.0)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch_transformers) (1.11.15)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (7.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (0.14.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->pytorch_transformers) (1.12.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (2019.11.28)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch_transformers) (1.24.3)\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.3.3)\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (1.14.15)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch_transformers) (0.9.4)\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_transformers) (0.15.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.15.0,>=1.14.15->boto3->pytorch_transformers) (2.6.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install pytorch_transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cN3zjpIdi3dc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "import random\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm_notebook, trange\n",
    "\n",
    "\n",
    "from pytorch_transformers import (WEIGHTS_NAME, BertConfig, BertForSequenceClassification, BertTokenizer,\n",
    "                                  XLMConfig, XLMForSequenceClassification, XLMTokenizer, \n",
    "                                  XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer,\n",
    "                                  RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "\n",
    "from pytorch_transformers import AdamW, WarmupLinearSchedule\n",
    "\n",
    "# from utils import (convert_examples_to_features,\n",
    "#                         output_modes, processors)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F93_pIopz7BG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "args_bert = {\n",
    "    'data_dir': 'data_new_1/',\n",
    "    'model_type':  'bert',\n",
    "    'model_name': 'bert-base-uncased',\n",
    "    'task_name': 'binary',\n",
    "    'output_dir': \"outputs_bert-base-uncased_0.7_taskekant\",\n",
    "    'cache_dir': 'cache/',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "    'max_seq_length': 128,\n",
    "    'output_mode': 'classification',\n",
    "    'train_batch_size': 8,\n",
    "    'eval_batch_size': 1,\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 100,\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 1e-10,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_steps': 0,\n",
    "    'max_grad_norm': 1.0,\n",
    "\n",
    "    'logging_steps': 50,\n",
    "    'evaluate_during_training': False,\n",
    "    'save_steps': 5000,\n",
    "    'eval_all_checkpoints': True,\n",
    "\n",
    "    'overwrite_output_dir': False,\n",
    "    'reprocess_input_data': False,\n",
    "    'notes': 'Using Yelp Reviews dataset'\n",
    "}\n",
    "\n",
    "args_roberta = {\n",
    "    'data_dir': 'data_new_1/',\n",
    "    'model_type':  'roberta',\n",
    "    'model_name': 'roberta-base',\n",
    "    'task_name': 'binary',\n",
    "    'output_dir': 'outputs_robert-base-0.9_taskbc',\n",
    "    'cache_dir': 'cache_roberta/',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'fp16': False,\n",
    "    'fp16_opt_level': 'O1',\n",
    "    'max_seq_length': 128,\n",
    "    'output_mode': 'classification',\n",
    "    'train_batch_size': 16,\n",
    "    'eval_batch_size': 1,\n",
    "\n",
    "    'gradient_accumulation_steps': 1,\n",
    "    'num_train_epochs': 10,\n",
    "    'weight_decay': 0,\n",
    "    'learning_rate': 4e-8,\n",
    "    'adam_epsilon': 1e-8,\n",
    "    'warmup_steps': 1000,\n",
    "    'max_grad_norm': 1.0,\n",
    "\n",
    "    'logging_steps': 50,\n",
    "    'evaluate_during_training': False,\n",
    "    'save_steps': 5000,\n",
    "    'eval_all_checkpoints': True,\n",
    "\n",
    "    'overwrite_output_dir': False,\n",
    "    'reprocess_input_data': False,\n",
    "    'notes': 'Using Yelp Reviews dataset'\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Uzr2RwGLz7BL",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('args_bert.json', 'w') as f:\n",
    "    json.dump(args_bert, f)\n",
    "with open('args_roberta.json', 'w') as f:\n",
    "    json.dump(args_roberta, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ymjmIyOhz7BN",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if os.path.exists(args_bert['output_dir']) and os.listdir(args_bert['output_dir']) and args_bert['do_train'] and not args_bert['overwrite_output_dir']:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args_bert['output_dir']))\n",
    "\n",
    "#Create roberta directory\n",
    "if os.path.exists(args_roberta['output_dir']) and os.listdir(args_roberta['output_dir']) and args_roberta['do_train'] and not args_roberta['overwrite_output_dir']:\n",
    "    raise ValueError(\"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(args_roberta['output_dir']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LAHYiiLMz7BP",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "}\n",
    "\n",
    "config_class_bert, model_class_bert, tokenizer_class_bert = MODEL_CLASSES[args_bert['model_type']]\n",
    "\n",
    "MODEL_CLASSES = {\n",
    "    'bert': (BertConfig, BertForSequenceClassification, BertTokenizer),\n",
    "    'xlnet': (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer),\n",
    "    'xlm': (XLMConfig, XLMForSequenceClassification, XLMTokenizer),\n",
    "    'roberta': (RobertaConfig, RobertaForSequenceClassification, RobertaTokenizer)\n",
    "}\n",
    "\n",
    "#roberta\n",
    "config_class_roberta, model_class_roberta, tokenizer_class_roberta = MODEL_CLASSES[args_roberta['model_type']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 642
    },
    "colab_type": "code",
    "id": "qm5AguwFz7BR",
    "outputId": "88af870f-3f6b-4ff1-b35e-b224b12b08c1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /tmp/tmpuy3a6qwg\n",
      "100%|██████████| 361/361 [00:00<00:00, 88879.06B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmpuy3a6qwg to cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpuy3a6qwg\n",
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": \"binary\",\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /tmp/tmp0jzyqseg\n",
      "100%|██████████| 231508/231508 [00:00<00:00, 1306231.92B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmp0jzyqseg to cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmp0jzyqseg\n",
      "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /root/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
     ]
    }
   ],
   "source": [
    "config_bert = config_class_bert.from_pretrained(args_bert['model_name'], num_labels=4, finetuning_task=args_bert['task_name'])#, output_hidden_states=True)\n",
    "tokenizer_bert = tokenizer_class_bert.from_pretrained(args_bert['model_name'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "colab_type": "code",
    "id": "IGZHNvKAz7BU",
    "outputId": "60756e90-675b-40cf-d59c-d1d868fe6967",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /root/.cache/torch/pytorch_transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.8f56353af4a709bf5ff0fbc915d8f5b42bfff892cbb6ac98c3c45f481a03c685\n",
      "INFO:pytorch_transformers.modeling_utils:Model config {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 4,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:pytorch_transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /tmp/tmpbk8m39i_\n",
      "100%|██████████| 440473133/440473133 [00:12<00:00, 36372489.47B/s]\n",
      "INFO:pytorch_transformers.file_utils:copying /tmp/tmpbk8m39i_ to cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:pytorch_transformers.file_utils:creating metadata file for /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:pytorch_transformers.file_utils:removing temp file /tmp/tmpbk8m39i_\n",
      "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/aa1ef1aede4482d0dbcd4d52baad8ae300e60902e88fcb0bebdec09afd232066.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:pytorch_transformers.modeling_utils:Weights of BertForSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "INFO:pytorch_transformers.modeling_utils:Weights from pretrained model not used in BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "model_bert = model_class_bert.from_pretrained(args_bert['model_name'],num_labels=4)\n",
    "# for name, param in model_bert.named_parameters():\n",
    "# \tif 'classifier' not in name: # classifier layer\n",
    "# \t\tparam.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xyxKpk_6z7BW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_bert.to(device);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bsPmRyGE8GnR",
    "outputId": "68eec250-bb1a-42d3-e08a-2afaaa0e7e94",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xe4P94Bfz7Ba",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "task = args_bert['task_name']\n",
    "\n",
    "processor = processors[task]()\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xqr_fwM3z7Bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_and_cache_examples(task, tokenizer,argus, evaluate=False,undersample_scale_factor=1,data_type=\"dev\"):\n",
    "    args=argus.copy()\n",
    "    processor = processors[task]()\n",
    "    output_mode = args['output_mode']\n",
    "    \n",
    "    mode = 'dev' if evaluate else 'train'\n",
    "    cached_features_file = os.path.join(args['data_dir'], \"cached_\"+str(mode)+\"_\"+str(args['model_name'])+\"_\"+str(args['max_seq_length'])+\"_\"+str(task))\n",
    "    \n",
    "    if os.path.exists(cached_features_file) and not args['reprocess_input_data'] and False:\n",
    "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
    "        features = torch.load(cached_features_file)\n",
    "               \n",
    "    else:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n",
    "        label_list = processor.get_labels()\n",
    "        examples = processor.get_dev_examples(args['data_dir'],data_type) if evaluate else processor.get_train_examples(args['data_dir'])\n",
    "#         print(len(examples))\n",
    "        examples  = [example for example in examples]\n",
    "        #print((examples))\n",
    "        \n",
    "        features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer, output_mode,\n",
    "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),            # xlnet has a cls token at the end\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
    "            pad_on_left=bool(args['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n",
    "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0,\n",
    "            process_count=2)\n",
    "        \n",
    "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
    "        torch.save(features, cached_features_file)\n",
    "        \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oCul6vvCz7Bg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train(train_dataset, model, tokenizer,argus):\n",
    "    global writerobj\n",
    "    args=argus.copy()\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args['train_batch_size'])\n",
    "    \n",
    "    t_total = len(train_dataloader) // args['gradient_accumulation_steps'] * args['num_train_epochs']\n",
    "    \n",
    "    no_decay = ['bias', 'LayerNorm.weight']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args['weight_decay']},\n",
    "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])\n",
    "    scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args['warmup_steps'], t_total=t_total)\n",
    "    \n",
    "    if args['fp16']:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])\n",
    "        \n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
    "    logger.info(\"  Num Epochs = %d\", args['num_train_epochs'])\n",
    "    logger.info(\"  Total train batch size  = %d\", args['train_batch_size'])\n",
    "    logger.info(\"  Gradient Accumulation steps = %d\", args['gradient_accumulation_steps'])\n",
    "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
    "\n",
    "    global_step = 0\n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(int(args['num_train_epochs']), desc=\"Epoch\")\n",
    "    \n",
    "    for _ in train_iterator:\n",
    "        epoch_iterator = tqdm_notebook(train_dataloader, desc=\"Iteration\")\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            #print(model.bert(**inputs).shape)\n",
    "            loss = outputs[0]  # model outputs are always tuple in pytorch-transformers (see doc)\n",
    "            print(\"\\r%f\" % loss, end='')\n",
    "            #writerobj.add_scalar('Loss/train', float(loss), global_step)\n",
    "            \n",
    "\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "                \n",
    "               \n",
    "                \n",
    " \n",
    "            if args['fp16']:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n",
    "                \n",
    "            else:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "            \n",
    "            tr_loss += loss.item()\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "                \n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0:\n",
    "                    # Log metrics\n",
    "                    if args['evaluate_during_training']:  # Only evaluate when single GPU otherwise metrics may not average well\n",
    "                        results = evaluate(model, tokenizer)\n",
    "                        \n",
    "\n",
    "                    logging_loss = tr_loss\n",
    "                    \n",
    "\n",
    "                if args['save_steps'] > 0 and global_step % args['save_steps'] == 0:\n",
    "                    # Save model checkpoint\n",
    "                    output_dir = os.path.join(args['output_dir'], 'checkpoint')\n",
    "                    if not os.path.exists(output_dir):\n",
    "                        os.makedirs(output_dir)\n",
    "                    model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(output_dir)\n",
    "                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "            \n",
    "\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fXUP-xd8cqXy"
   },
   "outputs": [],
   "source": [
    "def evaluate_runtime(data,gt,tokenizer,prefix=\"\"):\n",
    "    eval_dataset = load_and_cache_examples_runtime(data,gt, tokenizer, undersample_scale_factor=1, evaluate=True)\n",
    "\n",
    "    args=args_bert\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "    print(\"Run time evaluation on progress!!\")\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    probs=None\n",
    "    net_input=None\n",
    "    featurelis=[]\n",
    "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "            tmp_eval_loss2, logits2 = outputs[:2]\n",
    "            \n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            probs= torch.nn.functional.softmax(logits2.detach().cpu(), dim=1)\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "            net_input=inputs['input_ids'].detach().cpu().numpy()\n",
    "            featurelis.append(model.bert(inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])[1].data.cpu().numpy())\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "            probs=np.append(probs,torch.nn.functional.softmax(logits2.detach().cpu(), dim=1), axis=0)\n",
    "            net_input=np.append(net_input,inputs['input_ids'].detach().cpu().numpy(),axis=0)\n",
    "            featurelis.append(model.bert(inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])[1].data.cpu().numpy())\n",
    "              \n",
    "\n",
    "    return probs,preds,featurelis#,out_label_ids,net_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tUvkEBZUz7Bk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, matthews_corrcoef, confusion_matrix, classification_report\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_mismatched(labels, preds):\n",
    "    mismatched = labels != preds\n",
    "    examples = processor.get_dev_examples(args_bert['data_dir'])\n",
    "    wrong = [i for (i, v) in zip(examples, mismatched) if v]\n",
    "    \n",
    "    return wrong\n",
    "\n",
    "def get_eval_report(labels, preds):\n",
    "    mcc = matthews_corrcoef(labels, preds)\n",
    "    print(classification_report(labels, preds))\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
    "    print(classification_report(labels, preds))\n",
    "   \n",
    "    return {\n",
    "        \"mcc\": mcc,\n",
    "        \"tp\": tp,\n",
    "        \"tn\": tn,\n",
    "        \"fp\": fp,\n",
    "        \"fn\": fn\n",
    "    }, get_mismatched(labels, preds)\n",
    "\n",
    "def compute_metrics(task_name, preds, labels):\n",
    "    assert len(preds) == len(labels)\n",
    "    return get_eval_report(labels, preds)\n",
    "\n",
    "def evaluate(model, argus, tokenizer, prefix=\"\",datatype=\"dev\"):\n",
    "    \n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    args=argus.copy()\n",
    "    eval_output_dir = args['output_dir']\n",
    "\n",
    "    results = {}\n",
    "    EVAL_TASK = args['task_name']\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(EVAL_TASK, tokenizer, args, undersample_scale_factor=1, evaluate=True,data_type=datatype)\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    #print(len(eval_dataset))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    probs=None\n",
    "    net_input=None\n",
    "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            feature=model.bert(inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])[1].cpu().numpy()\n",
    "            print(feature.shape)\n",
    "            tmp_eval_loss, logits = outputs[:2]\n",
    "\n",
    "            eval_loss += tmp_eval_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "        \n",
    "        if preds is None:\n",
    "            preds = logits.detach().cpu().numpy()\n",
    "            probs= preds.copy()\n",
    "            out_label_ids = inputs['labels'].detach().cpu().numpy()\n",
    "            net_input=inputs['input_ids'].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs['labels'].detach().cpu().numpy(), axis=0)\n",
    "            probs=np.append(probs,logits.detach().cpu().numpy(), axis=0)\n",
    "            net_input=np.append(net_input,inputs['input_ids'].detach().cpu().numpy(),axis=0)\n",
    "            \n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    if args['output_mode'] == \"classification\":\n",
    "        preds = np.argmax(preds, axis=1)\n",
    "    elif args['output_mode'] == \"regression\":\n",
    "        preds = np.squeeze(preds)\n",
    "    result, wrong = compute_metrics(EVAL_TASK, preds, out_label_ids)\n",
    "    results.update(result)\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return results, wrong,probs,preds,out_label_ids,net_input\n",
    "\n",
    "\n",
    "def extract_features(model, argus, tokenizer, prefix=\"\",datatype=\"dev\"):\n",
    "    \n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    args=argus.copy()\n",
    "    eval_output_dir = args['output_dir']\n",
    "\n",
    "    results = {}\n",
    "    EVAL_TASK = args['task_name']\n",
    "\n",
    "    eval_dataset = load_and_cache_examples(EVAL_TASK, tokenizer, args, undersample_scale_factor=1, evaluate=True,data_type=datatype)\n",
    "    if not os.path.exists(eval_output_dir):\n",
    "        os.makedirs(eval_output_dir)\n",
    "\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "\n",
    "    # Eval!\n",
    "    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    #print(len(eval_dataset))\n",
    "    logger.info(\"  Num examples = %d\", len(eval_dataset))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    probs=None\n",
    "    net_input=None\n",
    "    \n",
    "    #save data\n",
    "    all_features=[]\n",
    "    all_sentences=[]\n",
    "    all_labels=[]\n",
    "\n",
    "    for batch in tqdm_notebook(eval_dataloader, desc=\"Evaluating\"):\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # print(tokenizer.decode(batch[0])#.decode)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2] if args['model_type'] in ['bert', 'xlnet'] else None,  # XLM don't use segment_ids\n",
    "                      'labels':         batch[3]}\n",
    "            outputs = model(**inputs)\n",
    "            feature=model.bert(inputs[\"input_ids\"],inputs[\"token_type_ids\"],inputs[\"attention_mask\"])[1].cpu().numpy()\n",
    "            \n",
    "            all_features.append(feature[0])\n",
    "            all_sentences.append(feature)\n",
    "            all_labels.append(batch[3].cpu().numpy())\n",
    "\n",
    "    return all_features,all_sentences,all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MlaeXY9sz7Bm",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Train_bert\n",
    "if args_bert['do_train'] and False:\n",
    "    print(\"Training Bert!!\")\n",
    "    train_dataset = load_and_cache_examples(task, tokenizer_bert,args_bert,undersample_scale_factor=1)\n",
    "    #print(train_dataset)\n",
    "    global_step, tr_loss = train(train_dataset, model_bert, tokenizer_bert,args_bert)\n",
    "    logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1On6YjIULf7v",
    "outputId": "81d15152-b82d-4b7c-8d3d-36ac98549188",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Saving model checkpoint to outputs_bert-base-uncased_0.7_taskekant\n"
     ]
    }
   ],
   "source": [
    "#Save bert\n",
    "if args_bert['do_train']:\n",
    "    if not os.path.exists(args_bert['output_dir']):\n",
    "            os.makedirs(args_bert['output_dir'])\n",
    "    logger.info(\"Saving model checkpoint to %s\", args_bert['output_dir'])\n",
    "    \n",
    "    model_to_save = model_bert.module if hasattr(model_bert, 'module') else model_bert  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(args_bert['output_dir'])\n",
    "    tokenizer_bert.save_pretrained(args_bert['output_dir'])\n",
    "    torch.save(args_bert, os.path.join(args_bert['output_dir'], 'training_args.bin'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nfKV6fwii3ev"
   },
   "source": [
    "# Get test set predictions and probs for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 383,
     "referenced_widgets": [
      "9332f0618ef1455cb89a20f365af0c0d",
      "a64a7aa3ef5845bfa5569ee0664b384a",
      "be7bd9627f0649bd93b12dde8c496c7b",
      "8b64d91bace9417085d9fcde5724cfc1",
      "9f1733a695ec42d48b90a3234fc5608a",
      "2c0def01ad6949f8ae20f2ba6f4787bf",
      "3e806ac2918446dd88ca421249c33e60",
      "7ca25684cc284401b8148c558934126d",
      "97263cd3868244d4a90bd2e8abdcb63a",
      "7d644df58fe741db831c6a2fd87060e3",
      "21b19ec3be5e42109d60e908cacf23ee",
      "b527a7ad41114f5c996c8844725c2d0f",
      "e4848e9a13bd4035888042dd64285199",
      "b3cbf2005a33451abaa205fdfe9e7ac3",
      "723a6f0a0a0f427fb611344a4c60a442",
      "baaa2b1026d244069715bc58c2c34c78"
     ]
    },
    "colab_type": "code",
    "id": "nlIp1xZai3ex",
    "outputId": "0e9dc46f-624f-405b-f13e-04a5e27b8284",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluate the following checkpoints: ['outputs_bert-base-uncased_0.7_taskekant']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs_bert-base-uncased_0.7_taskekant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 184/184 [00:00<00:00, 1496.69it/s]\n",
      "INFO:__main__:Saving features into cached file data_new_1/cached_dev_bert-base-uncased_128_binary\n",
      "INFO:__main__:***** Running evaluation  *****\n",
      "INFO:__main__:  Num examples = 184\n",
      "INFO:__main__:  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9332f0618ef1455cb89a20f365af0c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=184, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Evaluate the following checkpoints: ['outputs_bert-base-uncased_0.7_taskekant']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "outputs_bert-base-uncased_0.7_taskekant\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 276/276 [00:00<00:00, 1518.61it/s]\n",
      "INFO:__main__:Saving features into cached file data_new_1/cached_dev_bert-base-uncased_128_binary\n",
      "INFO:__main__:***** Running evaluation  *****\n",
      "INFO:__main__:  Num examples = 276\n",
      "INFO:__main__:  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97263cd3868244d4a90bd2e8abdcb63a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=276, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "if args_bert['do_eval']:\n",
    "    checkpoints = [args_bert['output_dir']]\n",
    "    if args_bert['eval_all_checkpoints']:\n",
    "        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args_bert['output_dir']+ '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "        logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "    for checkpoint in checkpoints:\n",
    "        print(checkpoint)\n",
    "        if True:\n",
    "            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "            model = model_class_bert.from_pretrained(checkpoint)\n",
    "            model.to(device)\n",
    "            xtest, sentence,ytest= extract_features(model, args_bert,tokenizer_bert, prefix=global_step,datatype=\"test\")\n",
    "\n",
    "\n",
    "results = {}\n",
    "if args_bert['do_eval']:\n",
    "    checkpoints = [args_bert['output_dir']]\n",
    "    if args_bert['eval_all_checkpoints']:\n",
    "        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args_bert['output_dir']+ '/**/' + WEIGHTS_NAME, recursive=True)))\n",
    "        logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
    "    logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
    "    for checkpoint in checkpoints:\n",
    "        print(checkpoint)\n",
    "        if True:\n",
    "            global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n",
    "            model = model_class_bert.from_pretrained(checkpoint)\n",
    "            model.to(device)\n",
    "            xtrain, sentence,ytrain= extract_features(model, args_bert,tokenizer_bert, prefix=global_step,datatype=\"train\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HwhP1OWpep64"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Runtime execution based ideas\n",
    "def load_and_cache_examples_runtime(data,gt,tokenizer, evaluate=False, undersample_scale_factor=1,task=\"binary\"):\n",
    "    processor = processors[task]()\n",
    "    output_mode = args_bert['output_mode']\n",
    "    args=args_bert\n",
    "    \n",
    "    examples_new = []\n",
    "    count=0\n",
    "    for  line,lab in zip(data,gt):\n",
    "            guid = \"%s-%s\" % (\"test\", count)\n",
    "            text_a = line\n",
    "            label = lab\n",
    "            #print(text_a,label)\n",
    "            examples_new.append(\n",
    "                InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
    "            count+=1\n",
    "    \n",
    "    if True:\n",
    "        logger.info(\"Creating features from dataset file at %s\", args['data_dir'])\n",
    "        label_list = processor.get_labels()\n",
    "        examples = examples_new#processor.get_dev_examples(args['data_dir']) if evaluate else processor.get_train_examples(args['data_dir'])\n",
    "#         print(len(examples))\n",
    "        examples  = [example for example in examples]\n",
    "        #print((examples))\n",
    "        \n",
    "        features = convert_examples_to_features(examples, label_list, args['max_seq_length'], tokenizer, output_mode,\n",
    "            cls_token_at_end=bool(args['model_type'] in ['xlnet']),            # xlnet has a cls token at the end\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            cls_token_segment_id=2 if args['model_type'] in ['xlnet'] else 0,\n",
    "            pad_on_left=bool(args['model_type'] in ['xlnet']),                 # pad on the left for xlnet\n",
    "            pad_token_segment_id=4 if args['model_type'] in ['xlnet'] else 0,\n",
    "            process_count=2)\n",
    "        \n",
    "       \n",
    "        \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    if output_mode == \"classification\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.long)\n",
    "    elif output_mode == \"regression\":\n",
    "        all_label_ids = torch.tensor([f.label_id for f in features], dtype=torch.float)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "9z_EIFKe0KsN",
    "outputId": "b29b61a4-d657-4f8a-ff77-df66f1f157b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.76000   0.33929   0.46914        56\n",
      "           1    0.57778   0.86667   0.69333        90\n",
      "           2    0.30769   0.26667   0.28571        15\n",
      "           3    0.27273   0.13043   0.17647        23\n",
      "\n",
      "    accuracy                        0.56522       184\n",
      "   macro avg    0.47955   0.40076   0.40616       184\n",
      "weighted avg    0.57309   0.56522   0.52726       184\n",
      "\n",
      "XGB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0    0.55263   0.37500   0.44681        56\n",
      "           1    0.59690   0.85556   0.70320        90\n",
      "           2    0.50000   0.06667   0.11765        15\n",
      "           3    0.33333   0.21739   0.26316        23\n",
      "\n",
      "    accuracy                        0.56522       184\n",
      "   macro avg    0.49572   0.37865   0.38270       184\n",
      "weighted avg    0.54258   0.56522   0.52243       184\n",
      "\n",
      "(276, 768)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.model_selection import RandomizedSearchCV,StratifiedKFold\n",
    "from sklearn.dummy import DummyClassifier\n",
    "import pickle\n",
    "\n",
    "\n",
    "params = {\n",
    "        'min_child_weight': [1, 5, 10],\n",
    "        'gamma': [0.5, 1, 1.5, 2, 5],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "        'max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "ytrain=np.array(ytrain).ravel()\n",
    "ytest=np.array(ytest).ravel()\n",
    "\n",
    "\n",
    "class_weights = list(class_weight.compute_class_weight('balanced',np.unique(ytrain),np.array(ytrain)))\n",
    "w_array = np.ones(ytrain.shape[0], dtype = 'float')\n",
    "\n",
    "for i, val in enumerate(ytrain):\n",
    "    w_array[i] = class_weights[val-1]\n",
    "\n",
    "clf= SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, max_iter=100, random_state=42,class_weight=\"balanced\",warm_start=True)\n",
    "clf2=XGBClassifier(n_estimators=200,random_state=10,max_depth=3,learning_rate =0.1)\n",
    "\n",
    "clf.fit(np.array(xtrain),np.array(ytrain).astype(np.int32))\n",
    "clf2.fit(np.array(xtrain),np.array(ytrain).astype(np.int32))\n",
    "\n",
    "ypred=clf.predict(xtest)\n",
    "ypred2=clf2.predict(xtest)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"SGD\")\n",
    "print(classification_report(np.array(ytest).ravel(),ypred,digits=5))\n",
    "\n",
    "\n",
    "print(\"XGB\")\n",
    "print(classification_report(np.array(ytest).ravel(),ypred2,digits=5))\n",
    "\n",
    "if True:\n",
    "  files = open('xtrain.pkl', 'wb')\n",
    "\n",
    "  # dump information to that file\n",
    "  pickle.dump(xtrain, files)\n",
    "  files = open('ytrain.pkl', 'wb')\n",
    "\n",
    "  # dump information to that file\n",
    "  pickle.dump(ytrain, files)\n",
    "\n",
    "  files = open('xtest.pkl', 'wb')\n",
    "\n",
    "  # dump information to that file\n",
    "  pickle.dump(xtest, files)\n",
    "\n",
    "  files = open('ytest.pkl', 'wb')\n",
    "\n",
    "  # dump information to that file\n",
    "  pickle.dump(ytest, files)\n",
    "  files.close()\n",
    "  print(np.array(xtrain).shape)\n",
    "\n",
    "#change classifier here\n",
    "clfimp=clf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H0NViOMU-alD",
    "outputId": "da82a7fe-eb2c-4b5f-e985-c82f85450174"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘set2_bert_inter’: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir set2_bert_inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6_HWT93QdCJ4"
   },
   "source": [
    "# BERT LIME INTERPRETABILITY TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "referenced_widgets": [
      "65eb1e5ce3ff46ce92d59acbd4401038",
      "4073e7999dc84c1d94123e9ebb8d0661",
      "ecfa453c6624458b8890d1294d05c4f9",
      "8573bc87c5824f4db76393ba8ae370df",
      "af5c6f64f77b4056a76dd55826d0286b",
      "6b5dc9903d6f4e93938055c356fc1933",
      "62b702d4b3ac4ac8ad6c926413479ea3",
      "c401bed5ca924e5f8d7895368a59421f",
      "aa58a1aed9d343cf83426a43acbad429",
      "b493bd3b86864cd6903bf619af99db5f",
      "bb78420a5da6460eb5c36ff6565ddeb9",
      "735f045a9c6141e6a3318f7de97db6b6",
      "6a2f92eb6b884c489da30649b0224088",
      "92d84da16fdc4da390af48ce759b7a9d",
      "1ea64c5088454160966ed56475aab685",
      "9a1d30b493394fb6917e0d667d026b47",
      "73695784105646cf8c2dfe039512e1cd",
      "17e2e02d669245b7aaaa681541a33f86",
      "6917e1fad67c4dfd9bd129dd8b4af719",
      "388c374130a74295a44b1140760617c2",
      "9f46140e2fd94288a91ed453dafb1e53",
      "1f655703beaf4912a46939220076ad22",
      "11de8d0b4ad94933bd9eba6138c922a6",
      "4405e5aa9c604c2e8bd38cab6d9a7761",
      "4a7e84042d8441e58f0c73903b137b2a",
      "e232f746ee3440daaffb6204f1c0fcd7",
      "52c569db0db74fe79ff6a981b021b3d2"
     ]
    },
    "colab_type": "code",
    "id": "vuniKq3GdFmw",
    "outputId": "02fbd37a-44cd-4498-98f0-f88d24329d10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "This agent still can’t handle problems involving shapes which have shading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2522.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65eb1e5ce3ff46ce92d59acbd4401038",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "This agent revision has improved performance across all E sets, as this union algorithm is both simple and fairly general.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2401.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4073e7999dc84c1d94123e9ebb8d0661",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "The Dark Pixel Ratio methodmust be fine-tuned even more to solve the other problems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2707.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecfa453c6624458b8890d1294d05c4f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Other than that, the root mean square difference is alsoused in certain places.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2761.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8573bc87c5824f4db76393ba8ae370df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      ", this is unlike human behaviour which appearsto be logical but doesn’t use any logic as part of the reasoning strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2314.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af5c6f64f77b4056a76dd55826d0286b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "I was pleasantly surprised by the agents ability to transfer much of its capability from the previous problem sets to these new ones.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2377.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5dc9903d6f4e93938055c356fc1933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Test and Raven set show no improvement.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 3179.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b702d4b3ac4ac8ad6c926413479ea3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Additionally, linear patterns between constraints can be identified as well.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2810.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c401bed5ca924e5f8d7895368a59421f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "I also removed the ability to transform (translate) shapes, which was somewhat inefficient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2292.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa58a1aed9d343cf83426a43acbad429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Also the rules were re-sorted such thatrules with hard thresholds for RMS and ED were given priority in processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2501.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b493bd3b86864cd6903bf619af99db5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "It is only able to identify the pattern that ispresent horizontally.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2948.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb78420a5da6460eb5c36ff6565ddeb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "It pre-processes each of the lettered images independently, so it runs in linear time, where n is the lettered images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2383.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "735f045a9c6141e6a3318f7de97db6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "The remaining set D and E were unique and I couldn’t find how to solve them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2738.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a2f92eb6b884c489da30649b0224088",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Computationally, theagent’s cognition is similar to the previous submission.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2638.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d84da16fdc4da390af48ce759b7a9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "The agent solves CP-6 and CP-7, resulting in improvement on challenge set, however the test set performance remained same.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2323.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea64c5088454160966ed56475aab685",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "It is also not able to adapt to new problem domains, by transferring knowledge from previous domains.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2543.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a1d30b493394fb6917e0d667d026b47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Unchanged transformations only check for images to beidentical and are also similar to human cognition.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2611.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73695784105646cf8c2dfe039512e1cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "The limitations of the agent could be improved considerably by adding new rules to tackle skipped problems and solving CP-4 througha different solution logic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2144.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e2e02d669245b7aaaa681541a33f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "The agent cannot still track thetransformation involving overlapping of two images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2705.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6917e1fad67c4dfd9bd129dd8b4af719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Once the ratio was established,finding the answer was a question of setting the ratio with the most scarce difference.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2397.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388c374130a74295a44b1140760617c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "As such rule-11 was modified (red highlighted) to accommodate this change, resulting in rule 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2605.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f46140e2fd94288a91ed453dafb1e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "When compared to the highest score achieved till this submission, it solves oneproblem extra in Ravens problems in Set E and 9 problems in Set D Basic, TestRaven and Challenge together.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 1890.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f655703beaf4912a46939220076ad22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "For BP-2, the affine relationship is not straightforward.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2945.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11de8d0b4ad94933bd9eba6138c922a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Analysis revealed thatmost parts of the rules are valid except for comparison between A and D. Previously in problem BP-10, B and D were same, where as in CP-3, this is not trueleading to failure of the comparison.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 1838.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4405e5aa9c604c2e8bd38cab6d9a7761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Moreover,the agent is now a matured production system for solving BP’s with productionrules in long term memory for solving RPM’s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2311.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a7e84042d8441e58f0c73903b137b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "It uses the AND operation on the image arrays tofind out the overlapped image.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:01<00:00, 2878.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e232f746ee3440daaffb6204f1c0fcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(5000, 768)\n",
      "Most importantly the performance on test set and ravens setsimproved to 6/12 and the developed rule also solved BP-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/lime/lime_text.py:116: FutureWarning: split() requires a non-empty pattern match.\n",
      "  self.as_list = [s for s in splitter.split(self.raw) if s]\n",
      "INFO:__main__:Creating features from dataset file at data_new_1/\n",
      "100%|██████████| 5000/5000 [00:02<00:00, 2446.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run time evaluation on progress!!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52c569db0db74fe79ff6a981b021b3d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Evaluating', max=5000, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buffered data was truncated after reaching the output size limit."
     ]
    }
   ],
   "source": [
    "def get_probs_fts(features_bert):\n",
    "    global clfimp\n",
    "    return clfimp.predict_proba(np.array(features_bert))\n",
    "\n",
    "def callme(a,b=None):\n",
    "    proboab,preds,net_inputs=evaluate_runtime(a,[str(1) for i in a],tokenizer_bert,prefix=\"\")\n",
    "    net_inputs=np.array(net_inputs)\n",
    "    net_inputs=net_inputs.squeeze()\n",
    "    print(net_inputs.shape)\n",
    "    probs_new=clfimp.predict_proba(net_inputs)\n",
    "    return probs_new\n",
    "            \n",
    "    \n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "explainer = LimeTextExplainer(class_names=[\"0\",\"1\",\"2\",\"3\"])\n",
    "index=0\n",
    "for txt in test_df[\"text\"]:\n",
    "    print(txt)\n",
    "    exp = explainer.explain_instance(txt, callme, num_features=128,top_labels=4)\n",
    "    #exp.show_in_notebook(text=True)\n",
    "    exp.save_to_file(\"set2_bert_inter/\"+\"results_\"+str(index)+\".html\")\n",
    "    index+=1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "trainer_bert_edtech.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "21b19ec3be5e42109d60e908cacf23ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Evaluating",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b3cbf2005a33451abaa205fdfe9e7ac3",
      "max": 276,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e4848e9a13bd4035888042dd64285199",
      "value": 276
     }
    },
    "2c0def01ad6949f8ae20f2ba6f4787bf": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e806ac2918446dd88ca421249c33e60": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "723a6f0a0a0f427fb611344a4c60a442": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ca25684cc284401b8148c558934126d": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7d644df58fe741db831c6a2fd87060e3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8b64d91bace9417085d9fcde5724cfc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ca25684cc284401b8148c558934126d",
      "placeholder": "​",
      "style": "IPY_MODEL_3e806ac2918446dd88ca421249c33e60",
      "value": "100% 184/184 [00:05&lt;00:00, 31.24it/s]"
     }
    },
    "9332f0618ef1455cb89a20f365af0c0d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_be7bd9627f0649bd93b12dde8c496c7b",
       "IPY_MODEL_8b64d91bace9417085d9fcde5724cfc1"
      ],
      "layout": "IPY_MODEL_a64a7aa3ef5845bfa5569ee0664b384a"
     }
    },
    "97263cd3868244d4a90bd2e8abdcb63a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_21b19ec3be5e42109d60e908cacf23ee",
       "IPY_MODEL_b527a7ad41114f5c996c8844725c2d0f"
      ],
      "layout": "IPY_MODEL_7d644df58fe741db831c6a2fd87060e3"
     }
    },
    "9f1733a695ec42d48b90a3234fc5608a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "a64a7aa3ef5845bfa5569ee0664b384a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3cbf2005a33451abaa205fdfe9e7ac3": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b527a7ad41114f5c996c8844725c2d0f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_baaa2b1026d244069715bc58c2c34c78",
      "placeholder": "​",
      "style": "IPY_MODEL_723a6f0a0a0f427fb611344a4c60a442",
      "value": "100% 276/276 [00:08&lt;00:00, 32.18it/s]"
     }
    },
    "baaa2b1026d244069715bc58c2c34c78": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be7bd9627f0649bd93b12dde8c496c7b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "IntProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "Evaluating",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c0def01ad6949f8ae20f2ba6f4787bf",
      "max": 184,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9f1733a695ec42d48b90a3234fc5608a",
      "value": 184
     }
    },
    "e4848e9a13bd4035888042dd64285199": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
