{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data and Model Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code prepares TF-IDF commonly used WINGUS approaches. Please modify input_dir and output_file as per your local setup. For more details please look at https://boudinfl.github.io/pke/build/html/tutorials/training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "from string import punctuation\n",
    "\n",
    "from pke import compute_document_frequency\n",
    "\n",
    "# setting info in terminal\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# path to the collection of documents\n",
    "input_dir = '../train_data/document/test/'\n",
    "\n",
    "# path to the df weights dictionary, saved as a gzipped csv file\n",
    "output_file = \"../data/df_wingus_test.tsv.gz\"\n",
    "\n",
    "# stoplist are punctuation marks\n",
    "stoplist = list(punctuation)\n",
    "stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
    "\n",
    "# compute idf weights\n",
    "compute_document_frequency(input_dir=input_dir,\n",
    "                           output_file=output_file,\n",
    "                           extension='txt', # input file extension\n",
    "                           language='en', # language of the input files\n",
    "                           normalization=\"stemming\", # use porter stemmer\n",
    "                           stoplist=stoplist,  # stoplist\n",
    "                           delimiter='\\t',  # tab separated output\n",
    "                           n=20)  # compute n-grams up to 5-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING WINGUS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is responsible for training the WINGUS model. Please execute the prior cell before starting training. Also point to term frquencey file and gold dataset. Data preparation steps are in https://boudinfl.github.io/pke/build/html/tutorials/training.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "#trainining\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import pke\n",
    "\n",
    "# setting info in terminal\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# path to the collection of documents\n",
    "input_dir = '../train_data/document/train/'\n",
    "\n",
    "# path to the reference file\n",
    "reference_file = \"../train_data/gold-annotation/train_gold.txt\"\n",
    "\n",
    "# path to the df file\n",
    "df_file = \"../data/df_wingus_train.tsv.gz\"\n",
    "logging.info('Loading df counts from {}'.format(df_file))\n",
    "df_counts = pke.load_document_frequency_file(input_file=df_file,\n",
    "                                             delimiter='\\t')\n",
    "\n",
    "# path to the model, saved as a pickle\n",
    "output_mdl = \"../data/wingus-model.pickle\"\n",
    "\n",
    "pke.train_supervised_model(input_dir=input_dir,\n",
    "                           reference_file=reference_file,\n",
    "                           model_file=output_mdl,\n",
    "                           extension='txt',\n",
    "                           language='en',\n",
    "                           normalization=\"stemming\",\n",
    "                           df=df_counts,\n",
    "                           model=pke.supervised.WINGUS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This part of the code executes testing. Modify the paths to point to trained model and testset by changing path in line ```\n",
    "df=pd.read_csv(\"../train_data/tsv/test2.tsv\",delimiter=\"\\t\")```\n",
    "\n",
    "* Possible values for test files are test1.tsv for fold-1, test2.tsv for fold-2 respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pke\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "# create a Kea extractor and set the input language to English (used for\n",
    "# the stoplist in the candidate selection method)\n",
    "extractor = pke.supervised.Kea()\n",
    "#extractor = pke.supervised.WINGUS()\n",
    "\n",
    "# load the content of the document, here in CoreNLP XML format\n",
    "# the use_lemmas parameter allows to choose using CoreNLP lemmas or stems \n",
    "# computed using nltk\n",
    "extractor.load_document('../train_data/document/train/train.txt')\n",
    "\n",
    "# select the keyphrase candidates, for Kea the 1-3 grams that do not start or\n",
    "# end with a stopword.\n",
    "extractor.candidate_selection()\n",
    "\n",
    "# load the df counts\n",
    "df_counts = pke.load_document_frequency_file(input_file=\"../data/df_wingus_train.tsv.gz\",\n",
    "                                             delimiter='\\t')\n",
    "\n",
    "# weight the candidates using Kea model.\n",
    "extractor.candidate_weighting(model_file=\"../data/wingus-model.pickle\", df=df_counts)\n",
    "\n",
    "# print the n-highest (10) scored candidates\n",
    "allkeyphrases=[]\n",
    "# print the n-highest (10) scored candidates\n",
    "for (keyphrase, score) in extractor.get_n_best(n=10, stemming=False):\n",
    "    allkeyphrases.append(keyphrase)\n",
    "\n",
    "df=pd.read_csv(\"../train_data/tsv/test1.tsv\",delimiter=\"\\t\")\n",
    "texts=df[\"text\"]\n",
    "texts=[i.replace(\",\",\"\") for i in texts]\n",
    "labels=df[\"label\"]\n",
    "labels=[0 if i==0 else 1 for i in labels]\n",
    "\n",
    "overall_evidence=[]\n",
    "removed_text=[]\n",
    "for txt in texts:\n",
    "    evidence=[]\n",
    "    for phr in allkeyphrases :\n",
    "        #Fuzzy matching\n",
    "        if phr in txt and txt not in removed_text:\n",
    "            evidence.append(1)\n",
    "            removed_text.append(txt)\n",
    "    if evidence==[]:\n",
    "        evidence=[0]\n",
    "    overall_evidence.append(evidence)\n",
    "\n",
    "ypred=[Counter(i).most_common(1)[0][0] for i in overall_evidence]\n",
    "print(ypred)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(labels,ypred,digits=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
